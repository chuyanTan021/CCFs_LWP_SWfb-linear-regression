{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8aacf6-a970-491d-b578-b7ec4ae25931",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This module is for loading the required OBS data for LRM training, from MAC-LWP, CERES_EBAF-TOA_Ed4.1, and MERRA-2 Reanalysis.\n",
    "\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray\n",
    "import pandas\n",
    "import glob\n",
    "\n",
    "from scipy.stats import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "\n",
    "from area_mean import *\n",
    "from binned_cyFunctions5 import *\n",
    "from read_hs_file import read_var_mod\n",
    "from read_var_obs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac01143-4d9d-4a8e-a23a-4ace52890951",
   "metadata": {},
   "source": [
    "# calc_LRMobs_metric.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a71ba5-b010-4da5-96bd-fa0b5548842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This module is to get the obs data we need from read func: 'get_OBSLRMdata', and calculate for CCFs and the required Cloud properties; \n",
    "## Crop regions, Transform the data to be annually mean, binned array form;\n",
    "## Create the linear regression 2 & 4 regimes models from current climate sensitivity of cloud properties to the CCFs and save the data.\n",
    "\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from scipy.stats import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "# self_defined modules\n",
    "from area_mean import *\n",
    "from binned_cyFunctions5 import *\n",
    "from read_hs_file import read_var_mod\n",
    "from read_var_obs import *\n",
    "from get_LWPCMIP5data import *\n",
    "from get_LWPCMIP6data import *\n",
    "from get_OBSLRMdata import *\n",
    "from fitLRM_cy1 import *\n",
    "from fitLRM_cy2 import *\n",
    "from fitLRM_cy4 import *\n",
    "from useful_func_cy import *\n",
    "from calc_Radiation_LRM_1 import *\n",
    "from calc_Radiation_LRM_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f5c30-8c83-45d3-b008-c3a46f799d3c",
   "metadata": {
    "collapsed": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "jupyter": {
     "outputs_hidden": {
      "outputs_hidden": true,
      "source_hidden": true
     },
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/chuyan/Research/Cloud_CCFs_RMs/Course_objective_ana/read_var_obs.py:66: UserWarning: WARNING: valid_range not used since it\n",
      "cannot be safely cast to variable data type\n",
      "  tt1 = nc.num2date(tt[i], file.variables['time'].units,calendar = u'standard')  # cf.Datetime object: including yr, mon, day, hour, minute, second info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "[[[0.3376463  0.3376463  0.3376463  ... 0.3376463  0.3376463  0.3376463 ]\n",
      "  [0.34254223 0.34311178 0.343692   ... 0.34082732 0.34139964 0.3419718 ]\n",
      "  [0.31732705 0.31867892 0.3200711  ... 0.3133953  0.31468073 0.3159917 ]\n",
      "  ...\n",
      "  [0.32840103 0.3282132  0.32801944 ... 0.32895696 0.3287822  0.32860112]\n",
      "  [0.31479692 0.31471968 0.3146458  ... 0.315017   0.31493765 0.31486344]\n",
      "  [0.29702592 0.29702592 0.29702592 ... 0.29702592 0.29702592 0.29702592]]\n",
      "\n",
      " [[0.32060426 0.32060426 0.32060426 ... 0.32060426 0.32060426 0.32060426]\n",
      "  [0.28848466 0.28902537 0.28957465 ... 0.28687114 0.28740743 0.28794304]\n",
      "  [0.24156712 0.24262238 0.24370147 ... 0.23853074 0.2395203  0.24053104]\n",
      "  ...\n",
      "  [0.55581355 0.5575762  0.5593538  ... 0.5506113  0.55231833 0.5540736 ]\n",
      "  [0.559898   0.56035906 0.56083226 ... 0.55844355 0.55893636 0.55942273]\n",
      "  [0.53008854 0.53008854 0.53008854 ... 0.53008854 0.53008854 0.53008854]]\n",
      "\n",
      " [[0.26042917 0.26042917 0.26042917 ... 0.26042917 0.26042917 0.26042917]\n",
      "  [0.32095447 0.32170862 0.32246068 ... 0.31865114 0.31942806 0.32019198]\n",
      "  [0.35436273 0.3555065  0.3566522  ... 0.35096422 0.35209095 0.3532214 ]\n",
      "  ...\n",
      "  [1.0592505  1.0586797  1.0581121  ... 1.0609925  1.0604123  1.059821  ]\n",
      "  [1.0309662  1.0308002  1.0305915  ... 1.0314857  1.0313417  1.0311531 ]\n",
      "  [0.9597589  0.9597589  0.9597589  ... 0.9597589  0.9597589  0.9597589 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.12626866 0.12626866 0.12626866 ... 0.12626866 0.12626866 0.12626866]\n",
      "  [0.14340669 0.1436513  0.1438908  ... 0.142656   0.14290623 0.14315562]\n",
      "  [0.1630442  0.16357216 0.16409951 ... 0.1614542  0.16198628 0.16251478]\n",
      "  ...\n",
      "  [0.84192204 0.84047323 0.8390031  ... 0.84613717 0.8447558  0.843338  ]\n",
      "  [0.8003614  0.7991404  0.7979025  ... 0.8040195  0.80278826 0.80158186]\n",
      "  [0.7966005  0.7966005  0.7966005  ... 0.7966005  0.7966005  0.7966005 ]]\n",
      "\n",
      " [[0.14597002 0.14597002 0.14597002 ... 0.14597002 0.14597002 0.14597002]\n",
      "  [0.16068293 0.16073011 0.16077106 ... 0.16053304 0.16058423 0.16063109]\n",
      "  [0.17018145 0.17048813 0.17079608 ... 0.16926195 0.16955933 0.16987364]\n",
      "  ...\n",
      "  [0.7097287  0.7124188  0.71513414 ... 0.70175695 0.7043984  0.70704377]\n",
      "  [0.7457718  0.7467644  0.74772406 ... 0.74286515 0.74382615 0.7447953 ]\n",
      "  [0.73493797 0.73493797 0.73493797 ... 0.73493797 0.73493797 0.73493797]]\n",
      "\n",
      " [[0.12459105 0.12459105 0.12459105 ... 0.12459105 0.12459105 0.12459105]\n",
      "  [0.11634749 0.11632806 0.1162992  ... 0.11642163 0.11639966 0.1163741 ]\n",
      "  [0.10929028 0.10948849 0.10970522 ... 0.10869396 0.10888116 0.10908423]\n",
      "  ...\n",
      "  [0.82319546 0.8231501  0.82310766 ... 0.8233094  0.8232747  0.82322276]\n",
      "  [0.8168539  0.8164381  0.8160361  ... 0.81807315 0.81766367 0.8172568 ]\n",
      "  [0.80252105 0.80252105 0.80252105 ... 0.80252105 0.80252105 0.80252105]]]\n",
      " ended cropping \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/chuyan/Research/Cloud_CCFs_RMs/Course_objective_ana/useful_func_cy.py:178: RuntimeWarning: Mean of empty slice\n",
      "  annually_array[i,:,:] = nanmean(dict_annually_mean[variable_nas[v]][i*12+(13-int(times[0,1])):(i+1)*12+(13-int(times[0,1])),:,:], axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End monthly data binned.\n",
      "End annually data binned.\n"
     ]
    }
   ],
   "source": [
    "# get the variable:\n",
    "test_flag = 'test1'\n",
    "inputVar_obs = get_OBSLRM(test = test_flag)\n",
    "# ------------------------ \n",
    "# radiation code\n",
    "\n",
    "# ------------------------\n",
    "\n",
    "# Data processing\n",
    "# GMT: Global mean surface air Temperature (2-meter), Unit in K\n",
    "gmt = inputVar_obs['tas'] * 1.\n",
    "# SST: Sea Surface Temperature or skin- Temperature, Unit in K\n",
    "SST = inputVar_obs['sfc_T'] * 1.\n",
    "# Precip: Precipitation, Unit in mm day^-1 (convert from kg m^-2 s^-1)\n",
    "Precip = inputVar_obs['P'] * (24. * 60 * 60)\n",
    "# Eva: Evaporation, Unit in mm day^-1 (here use the latent heat flux from the sfc, unit convert from W m^-2 --> kg m^-2 s^-1 --> mm day^-1)\n",
    "lh_vaporization = (2.501 - (2.361 * 10**-3) * (SST - 273.15)) * 1e6  # the latent heat of vaporization at the surface Temperature\n",
    "Eva = inputVar_obs['E'] / lh_vaporization * (24. * 60 * 60)\n",
    "\n",
    "# MC: Moisture Convergence, represent the water vapor abundance, Unit in mm day^-1\n",
    "MC = Precip - Eva\n",
    "print(MC)\n",
    "\n",
    "# LTS: Lower Tropospheric Stability, Unit in K (the same as Potential Temperature):\n",
    "k = 0.286\n",
    "\n",
    "theta_700 = inputVar_obs['T_700'] * (100000. / 70000.)**k\n",
    "theta_skin = inputVar_obs['sfc_T'] * (100000. / inputVar_obs['sfc_P'])**k\n",
    "LTS_m = theta_700 - theta_skin  # LTS with np.nan\n",
    "\n",
    "#.. mask the place with np.nan value\n",
    "LTS_e = np.ma.masked_where(theta_700==np.nan, LTS_m)\n",
    "# print(LTS_e)\n",
    "\n",
    "Subsidence = inputVar_obs['sub']\n",
    "\n",
    "# define Dictionary to store: CCFs(4), gmt, other variables :\n",
    "dict0_var = {'gmt': gmt, 'SST': SST, 'p_e': MC, 'LTS': LTS_e, 'SUB': Subsidence}  #  ,'LWP': LWP, 'rsdt': Rsdt_pi, 'rsut': Rsut_pi, 'rsutcs': Rsutcs_pi, 'albedo' : Albedo_pi, 'albedo_cs': Albedo_cs_pi, 'alpha_cre': Alpha_cre_pi, \n",
    "\n",
    "# Crop the regions\n",
    "# crop the variables to the Southern Ocean latitude range: (40 ~ 85^o S)\n",
    "\n",
    "variable_nas = ['SST', 'p_e', 'LTS', 'SUB']\n",
    "\n",
    "dict1_SO, lat_merra2_so, lon_merra2_so = region_cropping(dict0_var, variable_nas, inputVar_obs['lat_merra2'], inputVar_obs['lon_merra2'], lat_range = [-85., -40.], lon_range = [-180., 180.])\n",
    "\n",
    "# Time-scale average\n",
    "# monthly mean (not changed)\n",
    "dict2_SO_mon = deepcopy(dict1_SO)\n",
    "\n",
    "# annually mean variable\n",
    "dict2_SO_yr = get_annually_dict(dict1_SO, ['gmt', 'SST', 'p_e', 'LTS', 'SUB'], inputVar_obs['times_merra2'], label = 'mon')\n",
    "\n",
    "# binned (spatial) avergae\n",
    "# Southern Ocean 5 * 5 degree bin box\n",
    "\n",
    "#..set are-mean range and define function\n",
    "s_range = arange(-90., 90., 5.) + 2.5  #..global-region latitude edge: (36)\n",
    "x_range = arange(-180., 180., 5.)  #..logitude sequences edge: number: 72\n",
    "y_range = arange(-85, -40., 5.) +2.5  #..southern-ocaen latitude edge: 9\n",
    "# binned Monthly variables:\n",
    "dict3_SO_mon_bin = {}\n",
    "\n",
    "for c in range(len(variable_nas)):\n",
    "\n",
    "    dict3_SO_mon_bin[variable_nas[c]] = binned_cySouthOcean5(dict2_SO_mon[variable_nas[c]], lat_merra2_so, lon_merra2_so)\n",
    "\n",
    "dict3_SO_mon_bin['gmt'] = binned_cyGlobal5(dict2_SO_mon['gmt'], inputVar_obs['lat_merra2'], lon_merra2_so)\n",
    "print(\"End monthly data binned.\")\n",
    "\n",
    "# binned Annually data:\n",
    "dict3_SO_yr_bin = get_annually_dict(dict3_SO_mon_bin, ['gmt', 'SST', 'p_e', 'LTS', 'SUB'], inputVar_obs['times_merra2'])\n",
    "print(\"End annually data binned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdeadb2-a7d3-4245-93ea-722c1aa4ed93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'S' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ecd3c86fc89c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mxbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mybins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_range\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mS_binned_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mybins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'S' is not defined"
     ]
    }
   ],
   "source": [
    "XX, YY = np.meshgrid(lon, lat, indexing='xy')\n",
    "#..Southern Ocean region from 85S 40S\n",
    "x_range = np.arange(-180., 183, 5.)   #..number:73\n",
    "y_range = np.arange(-85., -35, 5.)   #.. (9)\n",
    "\n",
    "xbins, ybins = len(x_range) - 1, len(y_range) - 1\n",
    "\n",
    "S_binned_array = np.zeros((S.shape[0], ybins, xbins))\n",
    "\n",
    "for i in np.arange(S.shape[0]):\n",
    "    S_time_step = S[i,:,:]\n",
    "\n",
    "    #..find and subtract the missing points\n",
    "    ind =  np.isnan(S[i,:,:]) == False\n",
    "    S_binned_time, xedge, yedge, binnumber = stats.binned_statistic_2d(XX[ind].ravel(),YY[ind].ravel(), values = S_time_step[ind].ravel(), statistic = 'mean', bins=[xbins, ybins], expand_binnumbers =True)\n",
    "\n",
    "    S_binned_array[i,:,:] = S_binned_time.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fd167-7712-4423-9fdd-8f20984a05be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5040b4b-133f-4f9d-9b94-ac1ffd2b9931",
   "metadata": {},
   "source": [
    "# useful_func_cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd71930-8fd4-4229-8e41-09ba153a2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_crop = deepcopy(dict_raw)\n",
    "\n",
    "ind_i = (lats >= min(lat_range)) & (lats <= max(lat_range))\n",
    "lats_crop = lats[ind_i]\n",
    "ind_j = (lons >= min(lon_range)) & (lons <= max(lon_range))\n",
    "lons_crop = lons[ind_j]\n",
    "print(lats_crop, lons_crop)\n",
    "\n",
    "xxfor v in range(len(variable_nas)):\n",
    "\n",
    "    if dict_crop[variable_nas[v]].ndim == 3:\n",
    "        dict_crop[variable_nas[v]] = dict_crop[variable_nas[v]][:, ind_i, ind_j]  # variable in dictionary is in 3-D shape\n",
    "    elif dict_crop[variable_nas[v]].ndim == 2:\n",
    "        dict_crop[variable_nas[v]] = dict_crop[variable_nas[v]][ind_i, ind_j]  # variable in dictionary is in 2-D shape\n",
    "\n",
    "    else:\n",
    "        print(' A not valid dimension value for this variable: ', variable_nas[v])\n",
    "        continue\n",
    "print(' Ended cropping.')\n",
    "return dict_crop, lats_crop, lons_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061116d-3bcd-4593-9c08-a10602d0af4b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp = 'piControl'\n",
    "\n",
    "# CMIP6: 31\n",
    "AWICM11MR = {'modn': 'AWI-CM-1-1-MR', 'consort': 'AWI', 'cmip': 'cmip6',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "BCCCSMCM2MR = {'modn': 'BCC-CSM2-MR', 'consort': 'BCC', 'cmip': 'cmip6',\n",
    "               'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "BCCESM1 = {'modn': 'BCC-ESM1', 'consort': 'BCC', 'cmip': 'cmip6',\n",
    "               'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "CAMSCSM1 = {'modn': 'CAMS-CSM1-0', 'consort': 'CAMS', 'cmip': 'cmip6',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "CMCCCM2SR5 = {'modn': 'CMCC-CM2-SR5', 'consort': 'CMCC', 'cmip': 'cmip6', \n",
    "             'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "CESM2 = {'modn': 'CESM2', 'consort': 'NCAR', 'cmip': 'cmip6',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "CESM2FV2 = {'modn': 'CESM2-FV2', 'consort': 'NCAR', 'cmip': 'cmip6',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "CESM2WACCM = {'modn': 'CESM2-WACCM', 'consort': 'NCAR', 'cmip': 'cmip6',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "CESM2WACCMFV2 = {'modn': 'CESM2-WACCM-FV2', 'consort': 'NCAR', 'cmip': 'cmip6',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "\n",
    "CNRMCM61 = {'modn': 'CNRM-CM6-1', 'consort': 'CNRM-CERFACS', 'cmip': 'cmip6', \n",
    "               'exper': exp, 'ensmem': 'r1i1p1f2', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "CNRMCM61HR = {'modn': 'CNRM-CM6-1-HR', 'consort': 'CNRM-CERFACS', 'cmip': 'cmip6',\n",
    "               'exper': exp, 'ensmem': 'r1i1p1f2', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "CNRMESM21 = {'modn': 'CNRM-ESM2-1', 'consort': 'CNRM-CERFACS', 'cmip': 'cmip6', \n",
    "                 'exper': exp, 'ensmem': 'r1i1p1f2', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "CanESM5 = {'modn': 'CanESM5', 'consort': 'CCCma', 'cmip': 'cmip6',\n",
    "               'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "E3SM10 = {'modn': 'E3SM-1-0', 'consort': 'E3SM-Project', 'cmip': 'cmip6',\n",
    "              'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "\n",
    "ECEarth3 = {'modn': 'EC-Earth3', 'consort': 'EC-Earth-Consortium', 'cmip': 'cmip6',\n",
    "       'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "ECEarth3Veg = {'modn': 'EC-Earth3-Veg', 'consort': 'EC-Earth-Consortium', 'cmip': 'cmip6',\n",
    "       'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "\n",
    "FGOALSg3 = {'modn': 'FGOALS-g3', 'consort': 'CAS', 'cmip': 'cmip6',\n",
    "                'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "GISSE21G = {'modn': 'GISS-E2-1-G', 'consort': 'NASA-GISS', 'cmip': 'cmip6',\n",
    "                'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "GISSE21H = {'modn': 'GISS-E2-1-H', 'consort': 'NASA-GISS', 'cmip': 'cmip6',\n",
    "                'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "GISSE22G = {'modn': 'GISS-E2-2-G', 'consort': 'NASA-GISS', 'cmip': 'cmip6',\n",
    "               'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "GFDLCM4 = {'modn': 'GFDL-CM4', 'consort': 'NOAA-GFDL', 'cmip': 'cmip6',\n",
    "           'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gr1', \"typevar\": 'Amon'}\n",
    "# HADGEM3 = {'modn': 'HadGEM3-GC31-LL', 'consort': 'MOHC', 'cmip': 'cmip6',\n",
    "#             'exper': 'piControl', 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}   #..missing 'wap' in 'piControl' exp(Daniel says that HadGEM3-GC31 not using p-level, so doesn't have variables on p-level\n",
    "INM_CM48 = {'modn': 'INM-CM4-8', 'consort': 'INM', 'cmip': 'cmip6', \n",
    "                'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gr1', \"typevar\": 'Amon'}\n",
    "IPSLCM6ALR = {'modn': 'IPSL-CM6A-LR', 'consort': 'IPSL', 'cmip': 'cmip6',\n",
    "                  'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "MIROCES2L = {'modn': 'MIROC-ES2L', 'consort': 'MIROC', 'cmip': 'cmip6',\n",
    "              'exper': exp, 'ensmem': 'r1i1p1f2', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "MIROC6 = {'modn': 'MIROC6', 'consort': 'MIROC', 'cmip': 'cmip6',\n",
    "              'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "MPIESM12LR = {'modn': 'MPI-ESM1-2-LR', 'consort': 'MPI-M', 'cmip': 'cmip6',\n",
    "                  'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "MRIESM20 = {'modn': 'MRI-ESM2-0', 'consort': 'MRI', 'cmip': 'cmip6',\n",
    "                'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "NESM3 = {'modn': 'NESM3', 'consort': 'NUIST', 'cmip': 'cmip6', \n",
    "                 'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "NorESM2MM = {'modn': 'NorESM2-MM', 'consort': 'NCC', 'cmip': 'cmip6',\n",
    "                 'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "SAM0 = {'modn': 'SAM0-UNICON', 'consort': 'SNU', 'cmip': 'cmip6', \n",
    "            'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "TaiESM1 = {'modn': 'TaiESM1', 'consort': 'AS-RCEC', 'cmip': 'cmip6', \n",
    "                 'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "# CMIP5: 11\n",
    "ACCESS10 = {'modn': 'ACCESS1-0', 'consort': 'CSIRO-BOM', 'cmip': 'cmip5',   # 2-d (145) and 3-d (146) variables have different lat shape\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "ACCESS13 = {'modn': 'ACCESS1-3', 'consort': 'CSIRO-BOM', 'cmip': 'cmip5',   # 2-d (145) and 3-d (146) variables have different lat shape\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "BNUESM = {'modn': 'BNU-ESM', 'consort': 'BNU', 'cmip': 'cmip5',\n",
    "          'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "\n",
    "CCSM4 = {'modn': 'CCSM4', 'consort': 'NCAR', 'cmip': 'cmip5',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "CNRMCM5 = {'modn': 'CNRM-CM5', 'consort': 'CNRM-CERFACS', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "CSIRO_Mk360 = {'modn': 'CSIRO-Mk3-6-0', 'consort': 'CSIRO-QCCCE', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "CanESM2 = {'modn': 'CanESM2', 'consort': 'CCCma', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "FGOALSg2 = {'modn': 'FGOALS-g2', 'consort': 'LASG-CESS', 'cmip': 'cmip5',   # missing 'prw' in piControl\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "FGOALSs2 = {'modn': 'FGOALS-s2', 'consort': 'LASG-IAP', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "GFDLCM3 = {'modn': 'GFDL-CM3', 'consort': 'NOAA-GFDL', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "GISSE2H = {'modn': 'GISS-E2-H', 'consort': 'NASA-GISS', 'cmip': 'cmip5',\n",
    "           'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "GISSE2R = {'modn': 'GISS-E2-R', 'consort': 'NASA-GISS', 'cmip': 'cmip5',\n",
    "           'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "IPSLCM5ALR = {'modn': 'IPSL-CM5A-LR', 'consort': 'IPSL', 'cmip': 'cmip5',\n",
    "               'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "MIROC5 = {'modn': 'MIROC5', 'consort': 'MIROC', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "MPIESMMR = {'modn': 'MPI-ESM-MR', 'consort': 'MPI-M', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "NorESM1M = {'modn': 'NorESM1-M', 'consort': 'NCC', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "\n",
    "deck2 = [BCCESM1, CanESM5, CESM2, CESM2FV2, CESM2WACCM, CNRMESM21, GISSE21G, GISSE21H, IPSLCM6ALR, MRIESM20, MIROC6, SAM0, E3SM10, FGOALSg3, GFDLCM4, CAMSCSM1, INM_CM48, MPIESM12LR, AWICM11MR, BCCCSMCM2MR, CMCCCM2SR5, CESM2WACCMFV2, CNRMCM61, CNRMCM61HR, ECEarth3, ECEarth3Veg, GISSE22G, MIROCES2L, NESM3, NorESM2MM, TaiESM1, BNUESM, CCSM4, CNRMCM5, CSIRO_Mk360, CanESM2, FGOALSg2, FGOALSs2, GFDLCM3, GISSE2H, GISSE2R, IPSLCM5ALR, MIROC5, MPIESMMR, NorESM1M]   # current # 31 (no.19) + 14 = 45\n",
    "\n",
    "\n",
    "deck_nas2 = ['BCCESM1', 'CanESM5', 'CESM2', 'CESM2FV2', 'CESM2WACCM', 'CNRMESM2', 'GISSE21G', 'GISSE21H', 'IPSLCM6ALR', 'MRIESM20', 'MIROC6', 'SAM0', 'E3SM10', 'FGOALSg3', 'GFDLCM4', 'CAMSCSM1', 'INM_CM48', 'MPIESM12LR', 'AWICM11MR', 'BCCCSMCM2MR', 'CMCCCM2SR5', 'CESM2WACCMFV2', 'CNRMCM61', 'CNRMCM61HR', 'ECEarth3', 'ECEarth3Veg', 'GISSE22G', 'MIROCES2L', 'NESM3', 'NorESM2MM', 'TaiESM1', 'BNUESM', 'CCSM4', 'CNRMCM5', 'CSIRO_Mk360', 'CanESM2', 'FGOALSg2', 'FGOALSs2', 'GFDLCM3', 'GISSE2H', 'GISSE2R', 'IPSLCM5ALR', 'MIROC5', 'MPIESMMR', 'NorESM1M']  # current # 31 (np.19) + 14 = 45\n",
    "\n",
    "\n",
    "# get cmip6 data:\n",
    "name_j = 0\n",
    "while name_j < len(deck_nas2):\n",
    "\n",
    "    if modn == deck_nas2[name_j]:\n",
    "        if (deck2[name_j]['cmip']=='cmip6') & (type_analysis == 'forecasting'):\n",
    "            inputVar_pi, inputVar_abr = get_LWPCMIP6(**deck2[name_j])\n",
    "        elif (deck2[name_j]['cmip']=='cmip5') & (type_analysis == 'forecasting'):\n",
    "            inputVar_pi, inputVar_abr = get_LWPCMIP5(**deck2[name_j])\n",
    "        else:       # port for historical analysis\n",
    "            print('not existing data within cmip5 and cmip6 storage in scratch.')\n",
    "        break\n",
    "    print(\"Number of models: \", name_j)\n",
    "    name_j += 1\n",
    "\n",
    "#if name_j== len(deck_nas2) -1:\n",
    "#   print(\"Don't have this model right now !\")\n",
    "\n",
    "\n",
    "\n",
    "# begin process data:\n",
    "#..get the shapes of monthly data\n",
    "shape_lat = len(inputVar_pi['lat'])\n",
    "shape_lon = len(inputVar_pi['lon'])\n",
    "shape_time_pi = len(inputVar_pi['times'])\n",
    "shape_time_abr = len(inputVar_abr['times'])\n",
    "#print(shape_lat, shape_lon, shape_time_pi, shape_time_abr)\n",
    "\n",
    "\n",
    "#..choose lat 40 -85 °S as the Southern-Ocean Regions\n",
    "lons        = inputVar_pi['lon']\n",
    "lats        = inputVar_pi['lat'][:]\n",
    "\n",
    "levels      = array(inputVar_abr['pres'])\n",
    "times_abr   = inputVar_abr['times']\n",
    "times_pi    = inputVar_pi['times']\n",
    "\n",
    "lati0 = -40.\n",
    "latsi0= min(range(len(lats)), key = lambda i: abs(lats[i] - lati0))\n",
    "lati1 = -85.\n",
    "latsi1= min(range(len(lats)), key = lambda i: abs(lats[i] - lati1))\n",
    "print('lat index for 40.s; 85.s', latsi0, latsi1)\n",
    "\n",
    "\n",
    "shape_latSO = (latsi0 - latsi1) + 1 \n",
    "#print(shape_latSO)\n",
    "\n",
    "\n",
    "#..abrupt4xCO2 Variables: LWP, tas(gmt), SST, p-e, LTS, subsidence\n",
    "LWP_abr  = array(inputVar_abr['clwvi']) - array(inputVar_abr['clivi'])   #..units in kg m^-2\n",
    "\n",
    "gmt_abr  = array(inputVar_abr['tas'])\n",
    "SST_abr  = array(inputVar_abr['sfc_T'])\n",
    "\n",
    "Precip_abr = array(inputVar_abr['P']) * (24.*60.*60.)   #.. Precipitation. Convert the units from kg m^-2 s^-1 -> mm*day^-1\n",
    "\n",
    "lh_vaporization_abr = (2.501 - (2.361 * 10**-3) * (SST_abr - 273.15)) * 1e6  # the latent heat of vaporization at the surface Temperature\n",
    "# Eva_abr2 = array(inputVar_abr['E']) * (24. * 60 * 60)\n",
    "Eva_abr1 = array(inputVar_abr['E']) / lh_vaporization_abr * (24. * 60 * 60)  #.. Evaporation, mm day^-1\n",
    "\n",
    "MC_abr = Precip_abr - Eva_abr1   #..Moisture Convergence calculated from abrupt4xCO2's P - E, Units in mm day^-1\n",
    "\n",
    "Twp_abr  = array(inputVar_abr['clwvi'])\n",
    "Iwp_abr  = array(inputVar_abr['clivi'])\n",
    "\n",
    "if np.min(LWP_abr)<-1e-5:\n",
    "    LWP_abr = Twp_abr\n",
    "    print('clwvi mislabeled')\n",
    "print('Abr simple global-mean-gmt(K): ', nanmean(gmt_abr))\n",
    "\n",
    "#..pi-Control Variables: LWP, tas(gmt), SST, p-e, LTS, subsidence\n",
    "LWP  = array(inputVar_pi['clwvi']) - array(inputVar_pi['clivi'])   #..units in kg m^-2\n",
    "\n",
    "gmt  = array(inputVar_pi['tas'])\n",
    "SST  = array(inputVar_pi['sfc_T'])\n",
    "\n",
    "Precip = array(inputVar_pi['P'])* (24.*60.*60.)    #..Precipitation. Convert the units from kg m^-2 s^-1 -> mm*day^-1\n",
    "\n",
    "lh_vaporization = (2.501 - (2.361 * 10**-3) * (SST - 273.15)) * 1e6  # the latent heat of vaporization at the surface Temperature\n",
    "Eva1 = array(inputVar_pi['E']) / lh_vaporization * (24. * 60 * 60)\n",
    "# Eva2 = array(inputVar_pi['E']) * (24.*60.*60.)   #..evaporation, mm day^-1\n",
    "\n",
    "MC = Precip - Eva1   #..Moisture Convergence calculated from pi-Control's P - E, Units in mm day^-1\n",
    "\n",
    "Twp  = array(inputVar_pi['clwvi'])\n",
    "Iwp  = array(inputVar_pi['clivi'])\n",
    "\n",
    "print('pi-C simple global mean-gmt(K): ', nanmean(gmt))\n",
    "if np.min(LWP)<-1e-5:\n",
    "    LWP = Twp\n",
    "    print('clwvi mislabeled')\n",
    "\n",
    "#..abrupt4xCO2\n",
    "# Lower Tropospheric Stability:\n",
    "k  = 0.286\n",
    "theta_700_abr  = array(inputVar_abr['T_700']) * (100000./70000.)** k\n",
    "theta_skin_abr = array(inputVar_abr['sfc_T']) * (100000./ array(inputVar_abr['sfc_P'])) **k\n",
    "LTS_m_abr  = theta_700_abr - theta_skin_abr\n",
    "\n",
    "#..Subtract the outliers in T_700 and LTS_m, 'nan' comes from missing T_700 data\n",
    "LTS_e_abr  = ma.masked_where(theta_700_abr >= 500, LTS_m_abr)\n",
    "\n",
    "\n",
    "# Meteorology Subsidence at 500 hPa, units in Pa s^-1:\n",
    "Subsidence_abr =  array(inputVar_abr['sub'])\n",
    "\n",
    "\n",
    "#..pi-Control \n",
    "# Lower Tropospheric Stability:\n",
    "theta_700  = array(inputVar_pi['T_700']) * (100000./70000.)** k\n",
    "theta_skin = array(inputVar_pi['sfc_T']) * (100000./ array(inputVar_pi['sfc_P'])) **k\n",
    "LTS_m  = theta_700 - theta_skin\n",
    "\n",
    "#..Subtract the outliers in T_700 and LTS_m \n",
    "LTS_e  = ma.masked_where(theta_700 >= 500, LTS_m)\n",
    "\n",
    "\n",
    "#..Meteological Subsidence at 500 hPa, units in Pa s^-1:\n",
    "Subsidence =  array(inputVar_pi['sub'])\n",
    "\n",
    "\n",
    "# define Dictionary to store: CCFs(4), gmt, other variables:\n",
    "dict0_PI_var = {'gmt': gmt, 'LWP': LWP, 'TWP': Twp, 'IWP': Iwp, 'SST': SST, 'p_e': MC, 'LTS': LTS_e, 'SUB': Subsidence\n",
    "                 ,'lat':lats, 'lon':lons, 'times': times_pi, 'pres':levels}\n",
    "\n",
    "dict0_abr_var = {'gmt': gmt_abr, 'LWP': LWP_abr, 'TWP': Twp_abr, 'IWP': Iwp_abr, 'SST': SST_abr, 'p_e': MC_abr, 'LTS': LTS_e_abr \n",
    "                 ,'SUB': Subsidence_abr, 'lat':lats, 'lon':lons, 'times': times_abr, 'pres':levels}\n",
    "\n",
    "\n",
    "\n",
    "# get the Annual-mean, Southern-Ocean region arrays\n",
    "\n",
    "datavar_nas = ['LWP', 'TWP', 'IWP', 'SST', 'p_e', 'LTS', 'SUB']   #..7 varisables except gmt (lon dimension diff)\n",
    "\n",
    "dict1_PI_yr  = {}\n",
    "dict1_abr_yr = {}\n",
    "\n",
    "shape_yr_pi  = shape_time_pi//12\n",
    "shape_yr_abr =  shape_time_abr//12\n",
    "\n",
    "\n",
    "layover_yr_abr =  zeros((len(datavar_nas), shape_yr_abr, shape_latSO, shape_lon))\n",
    "layover_yr_pi  =  zeros((len(datavar_nas), shape_yr_pi, shape_latSO, shape_lon))\n",
    "\n",
    "layover_yr_abr_gmt =  zeros((shape_yr_abr, shape_lat, shape_lon))\n",
    "layover_yr_pi_gmt =  zeros((shape_yr_pi, shape_lat, shape_lon))\n",
    "\n",
    "\n",
    "for a in range(len(datavar_nas)):\n",
    "\n",
    "    for i in range(shape_time_abr//12):\n",
    "        layover_yr_abr[a, i,:,:] =  nanmean(dict0_abr_var[datavar_nas[a]][i*12:(i+1)*12, latsi1:latsi0+1,:], axis=0)\n",
    "\n",
    "    dict1_abr_yr[datavar_nas[a]+'_yr'] =  layover_yr_abr[a,:]\n",
    "\n",
    "    for j in range(shape_time_pi//12):\n",
    "        layover_yr_pi[a, j,:,:]  = nanmean(dict0_PI_var[datavar_nas[a]][j*12:(j+1)*12,  latsi1:latsi0+1,:], axis=0)\n",
    "\n",
    "    dict1_PI_yr[datavar_nas[a]+'_yr'] =  layover_yr_pi[a,:]\n",
    "\n",
    "    print(datavar_nas[a], \" finish calculating annually-mean array\")   \n",
    "\n",
    "\n",
    "\n",
    "for i in range(shape_time_abr//12):\n",
    "\n",
    "    layover_yr_abr_gmt[i,:,:]  =  nanmean(dict0_abr_var['gmt'][i*12:(i+1)*12, :,:], axis=0)\n",
    "dict1_abr_yr['gmt_yr']  =   layover_yr_abr_gmt\n",
    "\n",
    "\n",
    "for j in range(shape_time_pi//12):\n",
    "    layover_yr_pi_gmt[j,:,:]  =   nanmean(dict0_PI_var['gmt'][j*12:(j+1)*12, :,:], axis=0)\n",
    "dict1_PI_yr['gmt_yr']  =   layover_yr_pi_gmt\n",
    "\n",
    "print('gmt', \" finsih calc annuallt mean gmt\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate 5*5 bin array for variables (LWP, CCFs) in Southern Ocean Region:\n",
    "\n",
    "#..set are-mean range and define function\n",
    "x_range  = arange(-180., 180., 5.) #..logitude sequences edge: number: 72\n",
    "s_range  = arange(-90., 90, 5.)+ 2.5 #..global-region latitude edge: (36)\n",
    "y_range  = arange(-85, -40., 5.) +2.5 #..southern-ocaen latitude edge: 9\n",
    "\n",
    "\n",
    "# Annually variables in bin box: \n",
    "lat_array  = lats[latsi1:latsi0+1]\n",
    "lon_array  = lons\n",
    "lat_array1 =  lats\n",
    "#..big storage dict\n",
    "dict1_PI_var   = {}\n",
    "dict1_abr_var  =  {}\n",
    "dict1_yr_bin_PI  = {}\n",
    "dict1_yr_bin_abr = {}\n",
    "\n",
    "\n",
    "for b in range(len(datavar_nas)):\n",
    "\n",
    "    dict1_yr_bin_abr[datavar_nas[b]+'_yr_bin']  =   binned_cySouthOcean5(dict1_abr_yr[datavar_nas[b]+'_yr'], lat_array, lon_array)\n",
    "    dict1_yr_bin_PI[datavar_nas[b]+'_yr_bin']   =  binned_cySouthOcean5(dict1_PI_yr[datavar_nas[b]+'_yr'], lat_array, lon_array)\n",
    "    print(datavar_nas[b], \" finished calculating annually-mean bin array\") \n",
    "\n",
    "dict1_yr_bin_abr['gmt_yr_bin']   =  binned_cyGlobal5(dict1_abr_yr['gmt_yr'], lat_array1, lon_array)\n",
    "dict1_yr_bin_PI['gmt_yr_bin']   =  binned_cyGlobal5(dict1_PI_yr['gmt_yr'], lat_array1, lon_array)\n",
    "\n",
    "print('gmt_yr_bin', \" finish calc annually-mean binned gmt\")\n",
    "\n",
    "dict1_abr_var['dict1_yr_bin_abr']  =  dict1_yr_bin_abr\n",
    "dict1_PI_var['dict1_yr_bin_PI']  = dict1_yr_bin_PI\n",
    "\n",
    "\n",
    "\n",
    "# Monthly variables (same as above):\n",
    "dict1_mon_bin_PI  = {}\n",
    "dict1_mon_bin_abr = {}\n",
    "\n",
    "for c in range(len(datavar_nas)):\n",
    "\n",
    "    dict1_mon_bin_abr[datavar_nas[c]+'_mon_bin'] =  binned_cySouthOcean5(dict0_abr_var[datavar_nas[c]][:, latsi1:latsi0+1,:], lat_array, lon_array)\n",
    "    dict1_mon_bin_PI[datavar_nas[c]+'_mon_bin'] = binned_cySouthOcean5(dict0_PI_var[datavar_nas[c]][:, latsi1:latsi0+1,:], lat_array, lon_array)\n",
    "    print(datavar_nas[c], \" finish calculating monthly-mean bin array\")\n",
    "\n",
    "dict1_mon_bin_abr['gmt_mon_bin'] = binned_cyGlobal5(dict0_abr_var['gmt'], lat_array1, lon_array)\n",
    "dict1_mon_bin_PI['gmt_mon_bin'] = binned_cyGlobal5(dict0_PI_var['gmt'], lat_array1, lon_array)\n",
    "\n",
    "print('gmt_mon_bin', \" finish calc monthly-mean binned gmt\")\n",
    "\n",
    "dict1_abr_var['dict1_mon_bin_abr']  = dict1_mon_bin_abr\n",
    "dict1_PI_var['dict1_mon_bin_PI']  = dict1_mon_bin_PI\n",
    "\n",
    "\n",
    "# input the shapes of year and month of pi&abr exper into the raw data dictionaries:\n",
    "dict1_abr_var['shape_yr'] = shape_yr_abr\n",
    "dict1_PI_var['shape_yr'] = shape_yr_pi\n",
    "dict1_abr_var['shape_mon'] = shape_time_abr\n",
    "dict1_PI_var['shape_mon'] = shape_time_pi\n",
    "\n",
    "\n",
    "\n",
    "# Output a dict for processing function in 'calc_LRM_metrics', stored the data dicts for PI and abr, with the model name_dict\n",
    "# C_dict =  {'dict0_PI_var':dict1_PI_var, 'dict0_abr_var':dict1_abr_var, 'model_data':model_data}    #..revised in Dec.30th, at 2021,, note the name.\n",
    "\n",
    "\n",
    "# Second step processing data\n",
    "\n",
    "# load annually-mean bin data\n",
    "dict1_yr_bin_PI  = dict1_PI_var['dict1_yr_bin_PI']\n",
    "dict1_yr_bin_abr  = dict1_abr_var['dict1_yr_bin_abr']\n",
    "\n",
    "\n",
    "# load monthly bin data\n",
    "dict1_mon_bin_PI  = dict1_PI_var['dict1_mon_bin_PI']\n",
    "dict1_mon_bin_abr  = dict1_abr_var['dict1_mon_bin_abr']\n",
    "\n",
    "# data array in which shapes?\n",
    "shape_yr_PI_3 = dict1_yr_bin_PI['LWP_yr_bin'].shape\n",
    "shape_yr_abr_3 = dict1_yr_bin_abr['LWP_yr_bin'].shape\n",
    "\n",
    "shape_yr_PI_gmt = dict1_yr_bin_PI['gmt_yr_bin'].shape\n",
    "shape_yr_abr_gmt = dict1_yr_bin_abr['gmt_yr_bin'].shape\n",
    "\n",
    "shape_mon_PI_3 = dict1_mon_bin_PI['LWP_mon_bin'].shape\n",
    "shape_mon_abr_3 = dict1_mon_bin_abr['LWP_mon_bin'].shape\n",
    "\n",
    "shape_mon_PI_gmt = dict1_mon_bin_PI['gmt_mon_bin'].shape\n",
    "shape_mon_abr_gmt = dict1_mon_bin_abr['gmt_mon_bin'].shape\n",
    "\n",
    "# flatten the data array for 'training' lrm  coefficience\n",
    "\n",
    "dict2_predi_fla_PI = {}\n",
    "dict2_predi_fla_abr = {}\n",
    "\n",
    "#..Ravel binned array /Standardized data ARRAY :\n",
    "for d in range(len(datavar_nas)):\n",
    "\n",
    "    dict2_predi_fla_PI[datavar_nas[d]] = dict1_mon_bin_PI[datavar_nas[d]+'_mon_bin'].flatten()\n",
    "    dict2_predi_fla_abr[datavar_nas[d]] = dict1_mon_bin_abr[datavar_nas[d]+'_mon_bin'].flatten()\n",
    "\n",
    "\n",
    "#..Use area_mean method, 'np.repeat' and 'np.tile' to reproduce gmt area-mean Array as the same shape as other flattened variables\n",
    "GMT_pi_mon  = area_mean(dict1_mon_bin_PI['gmt_mon_bin'], s_range,  x_range)   #..ALL in shape : shape_yr_abr(single dimension)\n",
    "## dict2_predi_fla_PI['gmt']  = GMT_pi.repeat(730)   # something wrong when calc dX_dTg(dCCFS_dgmt)\n",
    "GMT_abr_mon  = area_mean(dict1_mon_bin_abr['gmt_mon_bin'], s_range, x_range)   #..ALL in shape : shape_yr_abr(single dimension)\n",
    "## dict2_predi_fla_abr['gmt'] = GMT_abr.repeat(730)\n",
    "\n",
    "dict2_predi_fla_PI['gmt'] = dict1_mon_bin_PI['gmt_mon_bin'][:,1:11,:].flatten()\n",
    "dict2_predi_fla_abr['gmt'] = dict1_mon_bin_abr['gmt_mon_bin'][:,1:11,:].flatten()\n",
    "\n",
    "#  shape of flattened array:\n",
    "shape_fla_PI = dict2_predi_fla_PI['LWP'].shape\n",
    "shape_fla_abr = dict2_predi_fla_abr['LWP'].shape\n",
    "\n",
    "\n",
    "# For pluging in different sets of cut-off(TR_sst & TR_sub) into LRM(s):\n",
    "\n",
    "##  split cut-off: TR_sst and TR_sub for N1 and N2 slices in sort of self-defined (Mon)variable ranges\n",
    "\n",
    "YY_ay_gcm  = dict1_mon_bin_PI['SST_mon_bin']\n",
    "XX_ay_gcm  = dict1_mon_bin_PI['SUB_mon_bin']\n",
    "\n",
    "\n",
    "y_gcm = linspace(nanpercentile(YY_ay_gcm, 5), nanpercentile(YY_ay_gcm, 99), 31)   #..supposed to be changed, 31\n",
    "x_gcm = linspace(nanpercentile(XX_ay_gcm, 5), nanpercentile(XX_ay_gcm, 95), 22)   #.., 22\n",
    "\n",
    "print(\"slice SUB bound:  \", x_gcm)\n",
    "print(\"slice SST bound:  \", y_gcm)\n",
    "\n",
    "\n",
    "# define cut-off\n",
    "\n",
    "TR_sst =  full(len(y_gcm)-1, NaN)\n",
    "TR_sub =  full(len(x_gcm) -1, NaN)\n",
    "\n",
    "for c in arange(len(y_gcm)-1):\n",
    "    TR_sst[c]  = (y_gcm[c] + y_gcm[c+1]) /2. \n",
    "print(\"TR_sst : \", TR_sst)\n",
    "for f in arange(len(x_gcm) -1):\n",
    "    TR_sub[f]  = (x_gcm[f] + x_gcm[f+1]) /2.\n",
    "print(\"TR_sub : \",TR_sub )\n",
    "\n",
    "#..storage N1*N2 shape output result:\n",
    "s1  = zeros((len(TR_sst), len(TR_sub)))\n",
    "s2  = zeros((len(TR_sst), len(TR_sub)))\n",
    "s3  = zeros((len(TR_sst), len(TR_sub)))\n",
    "s4  = zeros((len(TR_sst), len(TR_sub)))  \n",
    "s5  = zeros((len(TR_sst), len(TR_sub)))   #.. for store training data R^2: coefficient of determination\n",
    "\n",
    "cut_off1  = zeros((len(TR_sst), len(TR_sub)))   #..2d, len(y_gcm)-1 * len(x_gcm)-1\n",
    "cut_off2  = zeros((len(TR_sst), len(TR_sub)))\n",
    "coefa = []\n",
    "coefb = []\n",
    "coefc =  []\n",
    "coefd = []\n",
    "\n",
    "# plug the cut-off into LRM tring function:\n",
    "for i in range(len(y_gcm)-1):\n",
    "    for j in range(len(x_gcm)-1):\n",
    "        s1[i,j], s2[i,j], s3[i,j], s4[i,j], s5[i,j], cut_off1[i,j], cut_off2[i,j], coef_a, coef_b, coef_c, coef_d = train_LRM_4(TR_sst[i], TR_sub[j], dict2_predi_fla_PI, dict2_predi_fla_abr, shape_fla_PI, shape_fla_abr)\n",
    "\n",
    "        print('number:', i+j+ 2)\n",
    "\n",
    "        coefa.append(coef_a)\n",
    "        coefb.append(coef_b)\n",
    "        coefc.append(coef_c)\n",
    "        coefd.append(coef_d)\n",
    "\n",
    "# find the least bias and its position:\n",
    "min_pedict_absbias_id = unravel_index(nanargmin(s1, axis=None), s1.shape)\n",
    "max_training_R2_id  = unravel_index(nanargmax(s5, axis=None),  s5.shape)\n",
    "\n",
    "TR_minabias_SST =  y_gcm[min_pedict_absbias_id[0]]\n",
    "TR_minabias_SUB =  x_gcm[min_pedict_absbias_id[1]]\n",
    "\n",
    "TR_maxR2_SST  = y_gcm[max_training_R2_id[0]]\n",
    "TR_maxR2_SUB  = x_gcm[max_training_R2_id[1]]\n",
    "\n",
    "\n",
    "\n",
    "# Storage data into .npz file for each GCMs\n",
    "WD = '/glade/scratch/chuyan/CMIP_output/'\n",
    "\n",
    "savez(WD+ modn+'__'+ 'STAT_pi+abr_'+'22x_31y_Aug30th', bound_y = y_gcm,bound_x = x_gcm, stats_1 = s1, stats_2 = s2, stats_3 = s3, stats_4 = s4, stats_5 = s5, cut_off1=cut_off1, cut_off2=cut_off2, TR_minabias_SST=TR_minabias_SST, TR_minabias_SUB=TR_minabias_SUB, TR_maxR2_SST=TR_maxR2_SST, TR_maxR2_SUB=TR_maxR2_SUB,  coef_a = coefa, coef_b = coefb, coef_c = coefc, coefd = coefd)\n",
    "return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc9e8c-fc86-4a6e-a8f9-df18e9dae137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03db3396-b7df-4d68-bef8-7fb10f5afd9e",
   "metadata": {},
   "source": [
    "# P - E calculation/plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c83f98-398a-4308-91a9-fdc09ef8a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 'piControl'\n",
    "# CMIP6 (30)\n",
    "AWICM11MR = {'modn': 'AWI-CM-1-1-MR', 'consort': 'AWI', 'cmip': 'cmip6',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "BCCCSMCM2MR = {'modn': 'BCC-CSM2-MR', 'consort': 'BCC', 'cmip': 'cmip6',   # T700 (ta) and sfc_T (ts) have different time shape\n",
    "               'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "BCCESM1 = {'modn': 'BCC-ESM1', 'consort': 'BCC', 'cmip': 'cmip6',\n",
    "               'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "CAMSCSM1 = {'modn': 'CAMS-CSM1-0', 'consort': 'CAMS', 'cmip': 'cmip6',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "CMCCCM2SR5 = {'modn': 'CMCC-CM2-SR5', 'consort': 'CMCC', 'cmip': 'cmip6', \n",
    "             'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "CESM2 = {'modn': 'CESM2', 'consort': 'NCAR', 'cmip': 'cmip6',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "CESM2FV2 = {'modn': 'CESM2-FV2', 'consort': 'NCAR', 'cmip': 'cmip6',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "CESM2WACCM = {'modn': 'CESM2-WACCM', 'consort': 'NCAR', 'cmip': 'cmip6',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "CESM2WACCMFV2 = {'modn': 'CESM2-WACCM-FV2', 'consort': 'NCAR', 'cmip': 'cmip6',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "\n",
    "CNRMCM61 = {'modn': 'CNRM-CM6-1', 'consort': 'CNRM-CERFACS', 'cmip': 'cmip6', \n",
    "               'exper': exp, 'ensmem': 'r1i1p1f2', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "CNRMCM61HR = {'modn': 'CNRM-CM6-1-HR', 'consort': 'CNRM-CERFACS', 'cmip': 'cmip6',\n",
    "               'exper': exp, 'ensmem': 'r1i1p1f2', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "CNRMESM21 = {'modn': 'CNRM-ESM2-1', 'consort': 'CNRM-CERFACS', 'cmip': 'cmip6', \n",
    "                 'exper': exp, 'ensmem': 'r1i1p1f2', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "CanESM5 = {'modn': 'CanESM5', 'consort': 'CCCma', 'cmip': 'cmip6',\n",
    "               'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "E3SM10 = {'modn': 'E3SM-1-0', 'consort': 'E3SM-Project', 'cmip': 'cmip6',\n",
    "              'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "\n",
    "ECEarth3 = {'modn': 'EC-Earth3', 'consort': 'EC-Earth-Consortium', 'cmip': 'cmip6',\n",
    "       'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "ECEarth3Veg = {'modn': 'EC-Earth3-Veg', 'consort': 'EC-Earth-Consortium', 'cmip': 'cmip6',\n",
    "       'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "\n",
    "FGOALSg3 = {'modn': 'FGOALS-g3', 'consort': 'CAS', 'cmip': 'cmip6',\n",
    "                'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "GISSE21G = {'modn': 'GISS-E2-1-G', 'consort': 'NASA-GISS', 'cmip': 'cmip6',\n",
    "                'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "GISSE21H = {'modn': 'GISS-E2-1-H', 'consort': 'NASA-GISS', 'cmip': 'cmip6',\n",
    "                'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "GISSE22G = {'modn': 'GISS-E2-2-G', 'consort': 'NASA-GISS', 'cmip': 'cmip6',\n",
    "               'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "GFDLCM4 = {'modn': 'GFDL-CM4', 'consort': 'NOAA-GFDL', 'cmip': 'cmip6',\n",
    "           'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gr1', \"typevar\": 'Amon'}\n",
    "# HADGEM3 = {'modn': 'HadGEM3-GC31-LL', 'consort': 'MOHC', 'cmip': 'cmip6',\n",
    "#             'exper': 'piControl', 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}   #..missing 'wap' in 'piControl' exp(Daniel says that HadGEM3-GC31 not using p-level, so doesn't have variables on p-level\n",
    "INM_CM48 = {'modn': 'INM-CM4-8', 'consort': 'INM', 'cmip': 'cmip6', \n",
    "                'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gr1', \"typevar\": 'Amon'}  #..data not available again \n",
    "IPSLCM6ALR = {'modn': 'IPSL-CM6A-LR', 'consort': 'IPSL', 'cmip': 'cmip6',\n",
    "                  'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gr', \"typevar\": 'Amon'}\n",
    "MIROCES2L = {'modn': 'MIROC-ES2L', 'consort': 'MIROC', 'cmip': 'cmip6',\n",
    "              'exper': exp, 'ensmem': 'r1i1p1f2', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "MIROC6 = {'modn': 'MIROC6', 'consort': 'MIROC', 'cmip': 'cmip6',\n",
    "              'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "MPIESM12LR = {'modn': 'MPI-ESM1-2-LR', 'consort': 'MPI-M', 'cmip': 'cmip6',\n",
    "                  'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "MRIESM20 = {'modn': 'MRI-ESM2-0', 'consort': 'MRI', 'cmip': 'cmip6',\n",
    "                'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "NESM3 = {'modn': 'NESM3', 'consort': 'NUIST', 'cmip': 'cmip6', \n",
    "                 'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "NorESM2MM = {'modn': 'NorESM2-MM', 'consort': 'NCC', 'cmip': 'cmip6',\n",
    "                 'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "SAM0 = {'modn': 'SAM0-UNICON', 'consort': 'SNU', 'cmip': 'cmip6', \n",
    "            'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "TaiESM1 = {'modn': 'TaiESM1', 'consort': 'AS-RCEC', 'cmip': 'cmip6', \n",
    "                 'exper': exp, 'ensmem': 'r1i1p1f1', 'gg': 'gn', \"typevar\": 'Amon'}\n",
    "\n",
    "# CMIP5: (14)\n",
    "ACCESS10 = {'modn': 'ACCESS1-0', 'consort': 'CSIRO-BOM', 'cmip': 'cmip5',   # 2-d (145) and 3-d (146) variables have different lat shape\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "ACCESS13 = {'modn': 'ACCESS1-3', 'consort': 'CSIRO-BOM', 'cmip': 'cmip5',   # 2-d (145) and 3-d (146) variables have different lat shape\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "BNUESM = {'modn': 'BNU-ESM', 'consort': 'BNU', 'cmip': 'cmip5',\n",
    "          'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "CCSM4 = {'modn': 'CCSM4', 'consort': 'NCAR', 'cmip': 'cmip5',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "CNRMCM5 = {'modn': 'CNRM-CM5', 'consort': 'CNRM-CERFACS', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "CSIRO_Mk360 = {'modn': 'CSIRO-Mk3-6-0', 'consort': 'CSIRO-QCCCE', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "CanESM2 = {'modn': 'CanESM2', 'consort': 'CCCma', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "FGOALSg2 = {'modn': 'FGOALS-g2', 'consort': 'LASG-CESS', 'cmip': 'cmip5',   # missing 'prw' in piControl\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "FGOALSs2 = {'modn': 'FGOALS-s2', 'consort': 'LASG-IAP', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "GFDLCM3 = {'modn': 'GFDL-CM3', 'consort': 'NOAA-GFDL', 'cmip': 'cmip5',\n",
    "            'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "GISSE2H = {'modn': 'GISS-E2-H', 'consort': 'NASA-GISS', 'cmip': 'cmip5',\n",
    "           'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "GISSE2R = {'modn': 'GISS-E2-R', 'consort': 'NASA-GISS', 'cmip': 'cmip5',\n",
    "           'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "IPSLCM5ALR = {'modn': 'IPSL-CM5A-LR', 'consort': 'IPSL', 'cmip': 'cmip5',\n",
    "               'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "MIROC5 = {'modn': 'MIROC5', 'consort': 'MIROC', 'cmip': 'cmip5',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "MPIESMMR = {'modn': 'MPI-ESM-MR', 'consort': 'MPI-M', 'cmip': 'cmip5',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "NorESM1M = {'modn': 'NorESM1-M', 'consort': 'NCC', 'cmip': 'cmip5',\n",
    "             'exper': exp, 'ensmem': 'r1i1p1', \"typevar\": 'Amon'}\n",
    "\n",
    "\n",
    "deck = [BCCESM1, CanESM5, CESM2, CESM2FV2, CESM2WACCM, CNRMESM21, GISSE21G, GISSE21H, IPSLCM6ALR, MRIESM20, MIROC6, SAM0, E3SM10, FGOALSg3, GFDLCM4, CAMSCSM1, INM_CM48, MPIESM12LR, AWICM11MR, CMCCCM2SR5, CESM2WACCMFV2, CNRMCM61, CNRMCM61HR, ECEarth3, ECEarth3Veg, GISSE22G, MIROCES2L, NESM3, NorESM2MM, TaiESM1, BNUESM, CCSM4, CNRMCM5, CSIRO_Mk360, CanESM2, FGOALSg2, FGOALSs2, GFDLCM3, GISSE2H, GISSE2R, IPSLCM5ALR, MIROC5, MPIESMMR, NorESM1M]  #..current # 30 + 14 (44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e583f83-9965-49d2-8a98-e1726ffde195",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputVar_p_e_abr, inputVar_p_e_pi = get_P_E_CMIP(deck)\n",
    "inputVar_p_e_obs = get_P_E_OBS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521172e4-d8d6-4ce9-b8c5-c1dd97ac36bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deck = [BNUESM, CCSM4, CNRMCM5, CSIRO_Mk360, CanESM2, FGOALSg2, FGOALSs2, GFDLCM3, GISSE2H, GISSE2R, IPSLCM5ALR, MIROC5, MPIESMMR, NorESM1M]\n",
    "\n",
    "\n",
    "# loop through the GCM deck:\n",
    "\n",
    "inputVar_gcm_abr = {}\n",
    "inputVar_gcm_pi = {}\n",
    "\n",
    "for i in range(len(deck)):\n",
    "\n",
    "    if deck[i]['cmip'] == 'cmip5':\n",
    "\n",
    "        #..abrupt4xCO2\n",
    "        deck[i]['exper'] = 'abrupt4xCO2'\n",
    "\n",
    "        TEST1_time= read_var_mod(varnm='pr', read_p =False, time1=[1,1,1], time2=[3349, 12, 31], **deck[i])[-1]\n",
    "        time1=[int(min(TEST1_time[:,0])),1,1]\n",
    "        time2=[int(min(TEST1_time[:,0]))+149, 12, 31]\n",
    "\n",
    "        print(\"retrieve time: \", time1, time2)\n",
    "\n",
    "        #.. read 'abrupt4xCO2' P and E:\n",
    "        P_abr  = read_var_mod(varnm='pr', read_p=False, time1= time1, time2= time2, **deck[i])[0]\n",
    "        E_abr,[],lat_abr_cmip5,lon_abr_cmip5,times_abr_cmip5 = read_var_mod(varnm='hfls', read_p=False, time1= time1, time2= time2, **deck[i])\n",
    "        sfc_T_abr  = read_var_mod(varnm='ts', read_p=False, time1= time1, time2= time2, **deck[i])[0]\n",
    "\n",
    "        inputVar_abr_cmip5 = {'P': P_abr, 'E': E_abr, 'sfc_T': sfc_T_abr, 'lat': lat_abr_cmip5, 'lon': lon_abr_cmip5, 'times': times_abr_cmip5}\n",
    "\n",
    "        inputVar_gcm_abr[deck[i]['modn']] = inputVar_abr_cmip5\n",
    "\n",
    "        #..pi-Control\n",
    "        deck[i]['exper'] = 'piControl'\n",
    "\n",
    "        if deck[i]['modn'] == 'IPSL-CM5A-LR':\n",
    "            deck[i]['ensmem'] = 'r1i1p1'\n",
    "            TEST2_time= read_var_mod(varnm='hfls', read_p= False, time1=[1,1,1], time2=[8000,12,31], **deck[i])[-1]\n",
    "            timep1=[int(min(TEST2_time[:,0])), 1, 1]   #.. max-799\n",
    "            timep2=[int(min(TEST2_time[:,0]))+98, 12, 31]  #.. max-750\n",
    "\n",
    "        else:\n",
    "\n",
    "            TEST2_time= read_var_mod(varnm='ps', read_p= False, time1=[1,1,1], time2=[8000,12,31], **deck[i])[-1]\n",
    "            timep1=[int(min(TEST2_time[:,0])),1,1]   #.. max-799\n",
    "            timep2=[int(min(TEST2_time[:,0]))+98, 12, 31]  #.. max-750\n",
    "\n",
    "        print (\"retrieve time: \", timep1, timep2)\n",
    "\n",
    "        #.. read 'piControl' P and E data:\n",
    "\n",
    "        P_pi  = read_var_mod(varnm='pr', read_p=False, time1= timep1, time2= timep2, **deck[i])[0]\n",
    "        #..Precipitation, Units in kg m^-2 s^-1 = mm * s^-1\n",
    "        E_pi, [],lat_pi_cmip5,lon_pi_cmip5,times_pi_cmip5 = read_var_mod(varnm='hfls', read_p=False, time1= timep1, time2= timep2, **deck[i])\n",
    "        #..Evaporations, Units in W m^-2 = J * m^-2 * s^-1\n",
    "        sfc_T_pi = read_var_mod(varnm='ts', read_p=False, time1= timep1, time2= timep2, **deck[i])[0]\n",
    "\n",
    "        inputVar_pi_cmip5 = {'P': P_pi, 'E': E_pi, 'sfc_T': sfc_T_pi, 'lat':lat_pi_cmip5, 'lon':lon_pi_cmip5, 'times': times_pi_cmip5}\n",
    "        inputVar_gcm_pi[deck[i]['modn']] = inputVar_pi_cmip5\n",
    "\n",
    "    elif deck[i]['cmip'] == 'cmip6':\n",
    "\n",
    "        #..abrupt4xCO2\n",
    "        deck[i]['exper'] = 'abrupt-4xCO2'\n",
    "\n",
    "        if deck[i]['modn'] == 'HadGEM3-GC31-LL':\n",
    "            deck[i]['ensmem'] = 'r1i1p1f3'\n",
    "\n",
    "            TEST1_time= read_var_mod(varnm='pr', read_p =False, time1=[1,1,15], time2=[3349, 12, 15], **deck[i])[-1]\n",
    "            time1=[int(min(TEST1_time[:,0])),1,15]\n",
    "            time2=[int(min(TEST1_time[:,0]))+149, 12, 15]\n",
    "\n",
    "        elif deck[i]['modn'] == 'EC-Earth3':\n",
    "            deck[i]['ensmem'] = 'r3i1p1f1'\n",
    "\n",
    "            TEST1_time= read_var_mod(varnm='pr', read_p =False, time1=[1,1,1], time2=[3349, 12, 31], **deck[i])[-1]\n",
    "            time1=[int(min(TEST1_time[:,0])),1,1]\n",
    "            time2=[int(min(TEST1_time[:,0]))+149, 12, 31]\n",
    "\n",
    "        else:\n",
    "\n",
    "            TEST1_time= read_var_mod(varnm='pr', read_p =False, time1=[1,1,1],time2=[3349, 12, 31], **deck[i])[-1]\n",
    "            time1=[int(min(TEST1_time[:,0])),1,1]\n",
    "            time2=[int(min(TEST1_time[:,0]))+149, 12, 31]\n",
    "\n",
    "        print(\"retrieve time: \", time1, time2)\n",
    "\n",
    "        #.. read 'abrupt-4xCO2' P and E: \n",
    "        P_abr  = read_var_mod(varnm='pr', read_p=False, time1= time1, time2= time2, **deck[i])[0]\n",
    "        E_abr, [],lat_abr_cmip6,lon_abr_cmip6,times_abr_cmip6 = read_var_mod(varnm='hfls', read_p=False, time1= time1, time2= time2, **deck[i])\n",
    "        sfc_T_abr  = read_var_mod(varnm='ts', read_p=False, time1= time1, time2= time2, **deck[i])[0]\n",
    "        inputVar_abr_cmip6 = {'P': P_abr, 'E': E_abr, 'sfc_T': sfc_T_abr, 'lat': lat_abr_cmip6, 'lon': lon_abr_cmip6, 'times': times_abr_cmip6}\n",
    "\n",
    "        inputVar_gcm_abr[deck[i]['modn']] = inputVar_abr_cmip6\n",
    "\n",
    "        #..pi-Control\n",
    "        deck[i]['exper'] = 'piControl'\n",
    "\n",
    "        if deck[i]['modn'] == 'HadGEM3-GC31-LL':\n",
    "            deck[i]['ensmem'] = 'r1i1p1f1'\n",
    "            TEST2_time= read_var_mod(varnm='ps', read_p =False, time1=[1,1,15], time2=[8000,12,15], **deck[i])[-1]\n",
    "            timep1=[int(min(TEST2_time[:,0])), 1,15]   #.. max-799\n",
    "            timep2=[int(min(TEST2_time[:,0]))+98, 12, 15]  #.. max-750\n",
    "\n",
    "        elif deck[i]['modn'] == 'EC-Earth3':\n",
    "            deck[i]['ensmem'] = 'r1i1p1f1'\n",
    "            TEST2_time= read_var_mod(varnm='ps', read_p =False, time1=[1,1,1], time2=[8000,12,31], **deck[i])[-1]\n",
    "            timep1=[int(min(TEST2_time[:,0])), 1, 1]   #.. max-799\n",
    "            timep2=[int(min(TEST2_time[:,0]))+98, 12, 31]  #.. max-750\n",
    "\n",
    "        elif deck[i]['modn'] == 'NESM3':\n",
    "            deck[i]['ensmem'] = 'r1i1p1f1'\n",
    "            TEST2_time= read_var_mod(varnm ='ta', read_p= True, time1=[1,1,1], time2=[8000,12,31], **deck[i])[-1]\n",
    "            timep1=[int(min(TEST2_time[:,0])), 1, 1]   #.. max-799\n",
    "            timep2=[int(min(TEST2_time[:,0]))+98, 12, 31]  #.. max-750\n",
    "\n",
    "        elif deck[i]['modn'] == 'CNRM-CM6-1':\n",
    "            deck[i]['ensmem'] = 'r1i1p1f2'\n",
    "            TEST2_time= read_var_mod(varnm='hfls', read_p =False, time1=[1,1,1], time2=[8000,12,31], **deck[i])[-1]\n",
    "            timep1=[int(min(TEST2_time[:,0])), 1, 1]   #.. max-799\n",
    "            timep2=[int(min(TEST2_time[:,0]))+98, 12, 31]  #.. max-750\n",
    "\n",
    "        else:\n",
    "            TEST2_time= read_var_mod(varnm='ps', read_p =False, time1=[1,1,1], time2=[8000,12,31], **deck[i])[-1]\n",
    "            timep1=[int(min(TEST2_time[:,0])),1,1]   #.. max-799\n",
    "            timep2=[int(min(TEST2_time[:,0]))+98, 12, 31]  #.. max-750\n",
    "\n",
    "        print (\"retrieve time: \", timep1, timep2)\n",
    "\n",
    "        #.. read 'piControl' P and E data: \n",
    "\n",
    "        P_pi  = read_var_mod(varnm='pr', read_p=False, time1= timep1, time2= timep2, **deck[i])[0]\n",
    "        #..Precipitation, Units in kg m^-2 s^-1 = mm * s^-1\n",
    "        E_pi, [],lat_pi_cmip6,lon_pi_cmip6,times_pi_cmip6 = read_var_mod(varnm='hfls', read_p=False, time1= timep1, time2= timep2, **deck[i])\n",
    "        #..Evaporations, Units in W m^-2 = J * m^-2 * s^-1\n",
    "        sfc_T_pi  = read_var_mod(varnm='ts', read_p=False, time1= timep1, time2= timep2, **deck[i])[0]\n",
    "        inputVar_pi_cmip6 = {'P': P_pi, 'E': E_pi, 'sfc_T': sfc_T_pi, 'lat': lat_pi_cmip6, 'lon': lon_pi_cmip6, 'times': times_pi_cmip6}\n",
    "\n",
    "        inputVar_gcm_pi[deck[i]['modn']] = inputVar_pi_cmip6\n",
    "\n",
    "\n",
    "print(inputVar_gcm_abr, inputVar_gcm_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe22d6aa-2559-4d48-b950-b48db2ae2571",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deck' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e4e739e925dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# calculate 'P - E':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# GCM(s) data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 'abrupt4xCO2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mPrecip_abr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputVar_p_e_abr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeck\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'modn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'P'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m24.\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60.\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60.\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#.. Precipitation. Convert the units from kg m^-2 s^-1 -> mm*day^-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deck' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# calculate 'P - E':\n",
    "# GCM(s) data\n",
    "for m in range(len(deck)):\n",
    "    # 'abrupt4xCO2'\n",
    "    Precip_abr = np.asarray(inputVar_p_e_abr[deck[m]['modn']]['P']) * (24.*60.*60.)   #.. Precipitation. Convert the units from kg m^-2 s^-1 -> mm*day^-1\n",
    "\n",
    "    lh_vaporization_abr = (2.501 - (2.361 * 10**-3) * (inputVar_p_e_abr[deck[m]['modn']]['sfc_T'] - 273.15)) * 1e6  # the latent heat of vaporization at the surface Temperature\n",
    "    # Eva_abr2 = array(inputVar_p_e_abr[deck[m]['modn']]['E']) * (24. * 60 * 60)\n",
    "    Eva_abr1 = array(inputVar_p_e_abr[deck[m]['modn']]['E']) / lh_vaporization_abr * (24. * 60 * 60)  #.. Evaporation, mm day^-1\n",
    "\n",
    "    MC_abr = Precip_abr - Eva_abr1   #..Moisture Convergence calculated from abrupt4xCO2's P - E, Units in mm day^-1\n",
    "\n",
    "    inputVar_p_e_abr[deck[m]['modn']]['MC'] =  MC_abr\n",
    "\n",
    "    # 'piControl'\n",
    "    Precip_pi = array(inputVar_p_e_pi[deck[m]['modn']]['P']) * (24.*60.*60.)    #..Precipitation. Convert the units from kg m^-2 s^-1 -> mm*day^-1\n",
    "\n",
    "    lh_vaporization_pi = (2.501 - (2.361 * 10**-3) * (inputVar_p_e_pi[deck[m]['modn']]['sfc_T'] - 273.15)) * 1e6  # the latent heat of vaporization at the surface Temperature\n",
    "    Eva_pi1 = array(inputVar_p_e_pi[deck[m]['modn']]['E']) / lh_vaporization_pi * (24. * 60 * 60)\n",
    "    # Eva_pi2 = array(inputVar_pi['E']) * (24.*60.*60.)   #..evaporation, mm day^-1\n",
    "\n",
    "    MC_pi = Precip_pi - Eva_pi1   #..Moisture Convergence calculated from pi-Control's P - E, Units in mm day^-1\n",
    "\n",
    "    inputVar_p_e_pi[deck[m]['modn']]['MC'] = MC_pi\n",
    "\n",
    "# OBS: MERRA-2 Re-analysis data\n",
    "SST_obs = inputVar_p_e_obs['sfc_T'] * 1.\n",
    "# Precip: Precipitation, Unit in mm day^-1 (convert from kg m^-2 s^-1)\n",
    "Precip_obs = inputVar_p_e_obs['P'] * (24. * 60 * 60)\n",
    "# Eva: Evaporation, Unit in mm day^-1 (here use the latent heat flux from the sfc, unit convert from W m^-2 --> kg m^-2 s^-1 --> mm day^-1)\n",
    "lh_vaporization_obs = (2.501 - (2.361 * 10**-3) * (SST_obs - 273.15)) * 1e6  # the latent heat of vaporization at the surface Temperature\n",
    "Eva_obs = inputVar_p_e_obs['E'] / lh_vaporization_obs * (24. * 60 * 60)\n",
    "\n",
    "# MC: Moisture Convergence, represent the water vapor abundance, Unit in mm day^-1\n",
    "MC_obs = Precip_obs - Eva_obs\n",
    "\n",
    "inputVar_p_e_obs['MC'] = MC_obs\n",
    "\n",
    "# calc the 'abr - mean(pi)' value of 'P - E':\n",
    "inputVar_p_e_abrminuspi = {}\n",
    "for h in range(len(deck)):\n",
    "    inputVar_p_e_abrminuspi[deck[h]['modn']+'MC'] = inputVar_p_e_abr[deck[h]['modn']]['MC'] - np.nanmean(inputVar_p_e_pi[deck[h]['modn']]['MC'], axis = 0)  # return be 3-D shape array: (length_time_abrupt4xCO2, lat, lon)\n",
    "    inputVar_p_e_abrminuspi[deck[h]['modn']+'lat'] = inputVar_p_e_abr[deck[h]['modn']]['lat']\n",
    "    inputVar_p_e_abrminuspi[deck[h]['modn']+'lon'] = inputVar_p_e_abr[deck[h]['modn']]['lon']\n",
    "    \n",
    "\n",
    "# do annually mean on the OBS data:\n",
    "# annually mean variable\n",
    "inputVar2_OBS_yr = get_annually_dict(inputVar_p_e_obs, ['MC'], inputVar_p_e_obs['times'], label = 'mon')\n",
    "\n",
    "# Plotting function:\n",
    "pth_plotting = '/glade/work/chuyan/Research/Cloud_CCFs_RMs/Course_objective_ana/plot_file/P_E/'\n",
    "PL_P_E_lat(inputVar_p_e_pi, inputVar_p_e_abrminuspi, inputVar2_OBS_yr, deck, pth_plotting)\n",
    "\n",
    "# Save the calculated P - E metrics:\n",
    "pth_data = '/glade/work/chuyan/Research/Cloud_CCFs_RMs/Course_objective_ana/data_file/P_E/'\n",
    "np.savez(pth_data+'_P_E_data(global)', GCM_p_e_abr = inputVar_p_e_abr, GCM_p_e_pi = inputVar_p_e_pi, GCM_p_e_abrminuspi = inputVar_p_e_abrminuspi, OBS_p_e = inputVar_p_e_obs, OBS_p_e_annually_mean = inputVar2_OBS_yr)\n",
    "\n",
    "relative_P_E = []\n",
    "for j in range(len(deck)):\n",
    "    data_gcm_DELTA_SO, lat_gcm_so, lon_gcm_so = region_cropping_var(data_gcm_DELTA[deck[j]['modn']+'MC'], data_gcm_DELTA[deck[j]['modn']+'lat'], data_gcm_DELTA[deck[j]['modn']+'lon'], lat_range = [-85., -40.], lon_range = [-180., 180.])\n",
    "\n",
    "    relative_P_E.append(np.nanmean(area_mean(data_gcm_DELTA_SO[12*140:12*150, :,:], lat_gcm_so, lon_gcm_so)))\n",
    "\n",
    "# Normalized array (# = 44)\n",
    "nor_SO_P_E = (np.asarray(relative_P_E) - np.mean(relative_P_E)) / np.std(relative_P_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82050f81-626b-4d8e-9233-c2409b805b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba0a2c79-630c-4fc4-8997-852ea7784730",
   "metadata": {},
   "source": [
    "# read_var_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a7976-4a61-4bab-9927-d99060361268",
   "metadata": {
    "tags": []
   },
   "source": [
    "## read_var_obs_MERRA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c203bd16-5928-4977-b966-b43e8e7a430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERRA2_path1 = '/glade/scratch/chuyan/obs_data/MERRA2_200.tavgM_2d_flx_Nx.200005.nc4.nc4?EFLUX,PRECTOT,time,lat,lon'\n",
    "MERRA2_path2 = '/glade/scratch/chuyan/obs_data/MERRA2_300.instM_3d_asm_Np.200402.nc4.nc4?OMEGA,T,time,lev,lat,lon'\n",
    "\n",
    "MERRA2_path3 = '/glade/scratch/chuyan/obs_data/MERRA2_300.instM_3d_asm_Np.200402.SUB.nc'\n",
    "e_with_p = nc.Dataset(MERRA2_path1, 'r')\n",
    "\n",
    "# OMEGA = nc.Dataset(MERRA2_path2, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b23770a-90e3-40cd-92ae-bb4053f1dc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[9.9999999e+14 9.9999999e+14 9.9999999e+14 ... 9.9999999e+14\n",
      "    9.9999999e+14 9.9999999e+14]\n",
      "   [9.9999999e+14 9.9999999e+14 9.9999999e+14 ... 9.9999999e+14\n",
      "    9.9999999e+14 9.9999999e+14]\n",
      "   [9.9999999e+14 9.9999999e+14 9.9999999e+14 ... 9.9999999e+14\n",
      "    9.9999999e+14 9.9999999e+14]\n",
      "   ...\n",
      "   [2.4553006e+02 2.4552245e+02 2.4551442e+02 ... 2.4555354e+02\n",
      "    2.4554572e+02 2.4553787e+02]\n",
      "   [2.4574846e+02 2.4574371e+02 2.4573907e+02 ... 2.4576274e+02\n",
      "    2.4575800e+02 2.4575311e+02]\n",
      "   [2.4593690e+02 2.4593690e+02 2.4593690e+02 ... 2.4593690e+02\n",
      "    2.4593690e+02 2.4593690e+02]]\n",
      "\n",
      "  [[9.9999999e+14 9.9999999e+14 9.9999999e+14 ... 9.9999999e+14\n",
      "    9.9999999e+14 9.9999999e+14]\n",
      "   [9.9999999e+14 9.9999999e+14 9.9999999e+14 ... 9.9999999e+14\n",
      "    9.9999999e+14 9.9999999e+14]\n",
      "   [9.9999999e+14 9.9999999e+14 9.9999999e+14 ... 9.9999999e+14\n",
      "    9.9999999e+14 9.9999999e+14]\n",
      "   ...\n",
      "   [2.4666000e+02 2.4665086e+02 2.4664162e+02 ... 2.4668684e+02\n",
      "    2.4667816e+02 2.4666898e+02]\n",
      "   [2.4660078e+02 2.4659639e+02 2.4659203e+02 ... 2.4661470e+02\n",
      "    2.4660997e+02 2.4660547e+02]\n",
      "   [2.4668057e+02 2.4668057e+02 2.4668057e+02 ... 2.4668057e+02\n",
      "    2.4668057e+02 2.4668057e+02]]\n",
      "\n",
      "  [[9.9999999e+14 9.9999999e+14 9.9999999e+14 ... 9.9999999e+14\n",
      "    9.9999999e+14 9.9999999e+14]\n",
      "   [9.9999999e+14 9.9999999e+14 9.9999999e+14 ... 9.9999999e+14\n",
      "    9.9999999e+14 9.9999999e+14]\n",
      "   [9.9999999e+14 9.9999999e+14 9.9999999e+14 ... 9.9999999e+14\n",
      "    9.9999999e+14 9.9999999e+14]\n",
      "   ...\n",
      "   [2.4859850e+02 2.4859151e+02 2.4858467e+02 ... 2.4861902e+02\n",
      "    2.4861195e+02 2.4860515e+02]\n",
      "   [2.4856804e+02 2.4856461e+02 2.4856117e+02 ... 2.4857805e+02\n",
      "    2.4857471e+02 2.4857130e+02]\n",
      "   [2.4862384e+02 2.4862384e+02 2.4862384e+02 ... 2.4862384e+02\n",
      "    2.4862384e+02 2.4862384e+02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[2.7289777e+02 2.7289777e+02 2.7289777e+02 ... 2.7289777e+02\n",
      "    2.7289777e+02 2.7289777e+02]\n",
      "   [2.7288889e+02 2.7288870e+02 2.7288846e+02 ... 2.7288940e+02\n",
      "    2.7288931e+02 2.7288904e+02]\n",
      "   [2.7287558e+02 2.7287527e+02 2.7287500e+02 ... 2.7287662e+02\n",
      "    2.7287628e+02 2.7287601e+02]\n",
      "   ...\n",
      "   [2.3263420e+02 2.3262239e+02 2.3261063e+02 ... 2.3266920e+02\n",
      "    2.3265758e+02 2.3264577e+02]\n",
      "   [2.3262440e+02 2.3261874e+02 2.3261299e+02 ... 2.3264192e+02\n",
      "    2.3263606e+02 2.3263025e+02]\n",
      "   [2.3262714e+02 2.3262714e+02 2.3262714e+02 ... 2.3262714e+02\n",
      "    2.3262714e+02 2.3262714e+02]]\n",
      "\n",
      "  [[2.6668353e+02 2.6668353e+02 2.6668353e+02 ... 2.6668353e+02\n",
      "    2.6668353e+02 2.6668353e+02]\n",
      "   [2.6670108e+02 2.6670078e+02 2.6670078e+02 ... 2.6670093e+02\n",
      "    2.6670099e+02 2.6670090e+02]\n",
      "   [2.6671738e+02 2.6671701e+02 2.6671710e+02 ... 2.6671750e+02\n",
      "    2.6671732e+02 2.6671735e+02]\n",
      "   ...\n",
      "   [2.4180150e+02 2.4179329e+02 2.4178517e+02 ... 2.4182663e+02\n",
      "    2.4181836e+02 2.4180988e+02]\n",
      "   [2.4189676e+02 2.4189304e+02 2.4188876e+02 ... 2.4190933e+02\n",
      "    2.4190520e+02 2.4190109e+02]\n",
      "   [2.4200267e+02 2.4200267e+02 2.4200267e+02 ... 2.4200267e+02\n",
      "    2.4200267e+02 2.4200267e+02]]\n",
      "\n",
      "  [[2.3581360e+02 2.3581360e+02 2.3581360e+02 ... 2.3581360e+02\n",
      "    2.3581360e+02 2.3581360e+02]\n",
      "   [2.3588148e+02 2.3588177e+02 2.3588214e+02 ... 2.3588043e+02\n",
      "    2.3588080e+02 2.3588116e+02]\n",
      "   [2.3595465e+02 2.3595543e+02 2.3595604e+02 ... 2.3595259e+02\n",
      "    2.3595335e+02 2.3595404e+02]\n",
      "   ...\n",
      "   [2.6135898e+02 2.6135834e+02 2.6135809e+02 ... 2.6136108e+02\n",
      "    2.6136029e+02 2.6135968e+02]\n",
      "   [2.6161899e+02 2.6161844e+02 2.6161819e+02 ... 2.6161990e+02\n",
      "    2.6161978e+02 2.6161938e+02]\n",
      "   [2.6187421e+02 2.6187421e+02 2.6187421e+02 ... 2.6187421e+02\n",
      "    2.6187421e+02 2.6187421e+02]]]]\n"
     ]
    }
   ],
   "source": [
    "f_raw = nc.Dataset(MERRA2_path2, 'r')\n",
    "# print(f_raw)\n",
    "data_raw = np.asarray(f_raw.variables['T'][:])\n",
    "\n",
    "print(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7851411b-b9c8-4abb-b3de-91c38f06c72b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 361, 576)\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(e_with_p['PRECTOT']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851a173-832b-4145-9e34-887da7d7df9e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.628836e-05\n"
     ]
    }
   ],
   "source": [
    "print(np.nanmean(e_with_p['EFLUX'][:]/2.45e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f8904-05e7-4114-a21d-fef4e5021802",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.01340793e-07]\n"
     ]
    }
   ],
   "source": [
    "print((area_mean(e_with_p['PRECTOT'][:] - (e_with_p['EFLUX'][:]/2.45e6), e_with_p['lat'], e_with_p['lon'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49248323-bc9e-4eb8-af1f-1f04ef324099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-- -- -- ... -- -- --]\n",
      "   [-- -- -- ... -- -- --]\n",
      "   [-- -- -- ... -- -- --]\n",
      "   ...\n",
      "   [244.71212768554688 244.68438720703125 244.65695190429688 ...\n",
      "    244.7976837158203 244.7687225341797 244.7400665283203]\n",
      "   [245.21359252929688 245.19570922851562 245.17819213867188 ...\n",
      "    245.27149963378906 245.25198364257812 245.23243713378906]\n",
      "   [245.7446746826172 245.73719787597656 245.7297821044922 ...\n",
      "    245.76724243164062 245.7598419189453 245.752197265625]]\n",
      "\n",
      "  [[-- -- -- ... -- -- --]\n",
      "   [-- -- -- ... -- -- --]\n",
      "   [-- -- -- ... -- -- --]\n",
      "   ...\n",
      "   [247.1370849609375 247.0863800048828 247.03564453125 ...\n",
      "    247.28883361816406 247.2383575439453 247.18783569335938]\n",
      "   [246.8199920654297 246.794189453125 246.76699829101562 ...\n",
      "    246.88038635253906 246.86029052734375 246.84042358398438]\n",
      "   [246.5972900390625 246.59014892578125 246.58294677734375 ...\n",
      "    246.61904907226562 246.61184692382812 246.6045379638672]]\n",
      "\n",
      "  [[-- -- -- ... -- -- --]\n",
      "   [-- -- -- ... -- -- --]\n",
      "   [-- -- -- ... -- -- --]\n",
      "   ...\n",
      "   [248.91348266601562 248.87728881835938 248.8413543701172 ...\n",
      "    249.0216827392578 248.98568725585938 248.94972229003906]\n",
      "   [248.70156860351562 248.68292236328125 248.6633758544922 ...\n",
      "    248.7501678466797 248.734130859375 248.71803283691406]\n",
      "   [248.56527709960938 248.55982971191406 248.55447387695312 ...\n",
      "    248.58119201660156 248.57601928710938 248.57064819335938]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[272.88873291015625 272.8883361816406 272.88787841796875 ...\n",
      "    272.88958740234375 272.88934326171875 272.8890075683594]\n",
      "   [272.8586730957031 272.85760498046875 272.85638427734375 ...\n",
      "    272.8612060546875 272.8601989746094 272.8592834472656]\n",
      "   [272.7996520996094 272.798583984375 272.79742431640625 ...\n",
      "    272.8037414550781 272.8023681640625 272.80096435546875]\n",
      "   ...\n",
      "   [232.7380828857422 232.6929168701172 232.6478729248047 ...\n",
      "    232.87295532226562 232.8280792236328 232.78314208984375]\n",
      "   [232.64427185058594 232.616943359375 232.58999633789062 ...\n",
      "    232.72955322265625 232.70115661621094 232.67283630371094]\n",
      "   [232.619873046875 232.61053466796875 232.60125732421875 ...\n",
      "    232.64768981933594 232.63839721679688 232.62908935546875]]\n",
      "\n",
      "  [[266.7008361816406 266.7008056640625 266.7006530761719 ...\n",
      "    266.7015075683594 266.7010192871094 266.700927734375]\n",
      "   [266.73516845703125 266.73468017578125 266.734130859375 ...\n",
      "    266.73614501953125 266.73602294921875 266.7356872558594]\n",
      "   [266.7695007324219 266.7691345214844 266.76861572265625 ...\n",
      "    266.770751953125 266.7704162597656 266.7699890136719]\n",
      "   ...\n",
      "   [241.58384704589844 241.55299377441406 241.52227783203125 ...\n",
      "    241.6768035888672 241.64576721191406 241.61474609375]\n",
      "   [241.70901489257812 241.6901092529297 241.67169189453125 ...\n",
      "    241.76914978027344 241.74900817871094 241.72900390625]\n",
      "   [241.89376831054688 241.8871307373047 241.88064575195312 ...\n",
      "    241.91346740722656 241.9068603515625 241.90023803710938]]\n",
      "\n",
      "  [[235.88172912597656 235.88226318359375 235.88278198242188 ...\n",
      "    235.88018798828125 235.8806610107422 235.8812255859375]\n",
      "   [236.03707885742188 236.038818359375 236.04061889648438 ...\n",
      "    236.03208923339844 236.0338592529297 236.03555297851562]\n",
      "   [236.22958374023438 236.2326202392578 236.2355499267578 ...\n",
      "    236.2193603515625 236.222900390625 236.226318359375]\n",
      "   ...\n",
      "   [260.57562255859375 260.5745544433594 260.573486328125 ...\n",
      "    260.58148193359375 260.57904052734375 260.5771789550781]\n",
      "   [261.0964660644531 261.0954284667969 261.0945739746094 ...\n",
      "    261.1009826660156 261.0993347167969 261.0977478027344]\n",
      "   [261.6185607910156 261.6181640625 261.61798095703125 ...\n",
      "    261.62042236328125 261.6198425292969 261.6192932128906]]]]\n"
     ]
    }
   ],
   "source": [
    "regrid_OMEGA = nc.Dataset(MERRA2_path3, 'r')\n",
    "\n",
    "data = regrid_OMEGA.variables['T'][:, :, ::-1, :]\n",
    "\n",
    "# print(np.asarray(regrid_OMEGA.variables['lat'][::-1]))\n",
    "lon = regrid_OMEGA.variables['lon'][:]\n",
    "\n",
    "lon2 = lon\n",
    "# convert latitude from 0 ~ 360 to -180 ~ 180\n",
    "lon2[lon2 > 180.] = lon2[lon2 > 180.] - 360.\n",
    "ind_sort = np.argsort(lon2)\n",
    "lon2 = lon2[ind_sort]\n",
    "# print(lon2)\n",
    "\n",
    "data1 = data[:, :, :, ind_sort]\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e86db-b818-4082-b4de-8f53b8b67cb1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-05-01\n"
     ]
    }
   ],
   "source": [
    "print(e_with_p.RangeBeginningDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d93afe-2320-40eb-90a3-15dbe638ef27",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T': <class 'netCDF4._netCDF4.Variable'>\n",
       " float32 T(time, lev, lat, lon)\n",
       "     long_name: air_temperature\n",
       "     units: K\n",
       "     _FillValue: 1000000000000000.0\n",
       "     missing_value: 1000000000000000.0\n",
       "     fmissing_value: 1000000000000000.0\n",
       "     vmax: 1000000000000000.0\n",
       "     vmin: -1000000000000000.0\n",
       "     valid_range: [-1.e+15  1.e+15]\n",
       "     origname: T\n",
       "     fullnamepath: /T\n",
       " unlimited dimensions: \n",
       " current shape = (1, 42, 361, 576)\n",
       " filling on,\n",
       " 'OMEGA': <class 'netCDF4._netCDF4.Variable'>\n",
       " float32 OMEGA(time, lev, lat, lon)\n",
       "     long_name: vertical_pressure_velocity\n",
       "     units: Pa s-1\n",
       "     _FillValue: 1000000000000000.0\n",
       "     missing_value: 1000000000000000.0\n",
       "     fmissing_value: 1000000000000000.0\n",
       "     vmax: 1000000000000000.0\n",
       "     vmin: -1000000000000000.0\n",
       "     valid_range: [-1.e+15  1.e+15]\n",
       "     origname: OMEGA\n",
       "     fullnamepath: /OMEGA\n",
       " unlimited dimensions: \n",
       " current shape = (1, 42, 361, 576)\n",
       " filling on,\n",
       " 'lat': <class 'netCDF4._netCDF4.Variable'>\n",
       " float64 lat(lat)\n",
       "     long_name: latitude\n",
       "     units: degrees_north\n",
       "     vmax: 1000000000000000.0\n",
       "     vmin: -1000000000000000.0\n",
       "     valid_range: [-1.e+15  1.e+15]\n",
       "     origname: lat\n",
       "     fullnamepath: /lat\n",
       " unlimited dimensions: \n",
       " current shape = (361,)\n",
       " filling on, default _FillValue of 9.969209968386869e+36 used,\n",
       " 'lev': <class 'netCDF4._netCDF4.Variable'>\n",
       " float64 lev(lev)\n",
       "     long_name: vertical level\n",
       "     units: hPa\n",
       "     positive: down\n",
       "     vmax: 1000000000000000.0\n",
       "     vmin: -1000000000000000.0\n",
       "     valid_range: [-1.e+15  1.e+15]\n",
       "     origname: lev\n",
       "     fullnamepath: /lev\n",
       " unlimited dimensions: \n",
       " current shape = (42,)\n",
       " filling on, default _FillValue of 9.969209968386869e+36 used,\n",
       " 'lon': <class 'netCDF4._netCDF4.Variable'>\n",
       " float64 lon(lon)\n",
       "     long_name: longitude\n",
       "     units: degrees_east\n",
       "     vmax: 1000000000000000.0\n",
       "     vmin: -1000000000000000.0\n",
       "     valid_range: [-1.e+15  1.e+15]\n",
       "     origname: lon\n",
       "     fullnamepath: /lon\n",
       " unlimited dimensions: \n",
       " current shape = (576,)\n",
       " filling on, default _FillValue of 9.969209968386869e+36 used,\n",
       " 'time': <class 'netCDF4._netCDF4.Variable'>\n",
       " int32 time(time)\n",
       "     long_name: time\n",
       "     units: minutes since 2015-10-01 00:00:00\n",
       "     time_increment: 60000\n",
       "     begin_date: 20151001\n",
       "     begin_time: 0\n",
       "     vmax: 1000000000000000.0\n",
       "     vmin: -1000000000000000.0\n",
       "     valid_range: [-1.e+15  1.e+15]\n",
       "     origname: time\n",
       "     fullnamepath: /time\n",
       " unlimited dimensions: \n",
       " current shape = (1,)\n",
       " filling on, default _FillValue of -2147483647 used}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OMEGA.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3119f-ca06-42fc-8803-42bc0dd24e89",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "int32 time(time)\n",
      "    long_name: time\n",
      "    units: minutes since 2015-10-01 00:00:00\n",
      "    time_increment: 60000\n",
      "    begin_date: 20151001\n",
      "    begin_time: 0\n",
      "    vmax: 1000000000000000.0\n",
      "    vmin: -1000000000000000.0\n",
      "    valid_range: [-1.e+15  1.e+15]\n",
      "    origname: time\n",
      "    fullnamepath: /time\n",
      "unlimited dimensions: \n",
      "current shape = (1,)\n",
      "filling on, default _FillValue of -2147483647 used\n"
     ]
    }
   ],
   "source": [
    "print(OMEGA.variables['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c969e3bc-24cf-4ffc-9251-8878a96c4e99",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/ssg/ch/usr/jupyterhub/envs/npl-3.7.9/dav/lib/python3.7/site-packages/ipykernel_launcher.py:51: UserWarning: WARNING: valid_range not used since it\n",
      "cannot be safely cast to variable data type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f5c6673fd183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# use 'np.concatenate' to get rid of one extra axis (the second axes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mdataOUT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mtimeOUT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Variable.__array__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Variable.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Variable._toma\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Variable._check_safecast\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/glade/u/ssg/ch/usr/jupyterhub/envs/npl-3.7.9/dav/lib/python3.7/site-packages/netCDF4/utils.py\u001b[0m in \u001b[0;36m_safecast\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_safecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# check to see if array a can be safely cast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# to array b.  A little less picky than numpy.can_cast.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pp_path_OBS='/glade/scratch/chuyan/obs_data/'\n",
    "\n",
    "valid_range1 = [2002, 7, 15]\n",
    "valid_range2 = [2022, 3, 31]\n",
    "read_p = True\n",
    "\n",
    "varnm = 'T'\n",
    "\n",
    "### ---------------\n",
    "# This function is for reading MERRA-2 Re-analysis dataset.\n",
    "# read_p = False: 2-D variable; True: 3-D variable (e.g., air Temerature, vetical Pressure velocity)\n",
    "# valid_range1 & 2: the starting and end time_stamps of time sequential metric. Only first two numbers are valid for monthly data.\n",
    "### ---------------\n",
    "\n",
    "# 'read_hs' functionality:\n",
    "# read the data file, according to the variable name: varnm and the data loading path: pp_path_OBS\n",
    "\n",
    "folder = pp_path_OBS\n",
    "if varnm in ['EFLUX', 'PRECTOT']:\n",
    "    fn = glob.glob(folder + '*'+ 'tavgM_2d_flx_Nx.' + '*' + 'nc4.nc4?' + 'EFLUX,PRECTOT' + '*')\n",
    "    # print(fn)\n",
    "elif varnm in ['OMEGA500', 'PS', 'QV10M', 'T2M', 'TS']:\n",
    "    fn = glob.glob(folder + '*'+ 'tavgM_2d_slv_Nx.' + '*' + 'nc4.nc4?' + 'OMEGA500,PS,QV10M,T2M,TS' + '*')\n",
    "    # print(fn)\n",
    "elif varnm in ['OMEGA', 'T']:\n",
    "    fn = glob.glob(folder + '*'+ 'instM_3d_asm_Np.' + '*' + 'nc4.nc4?' + 'OMEGA,T' + '*')\n",
    "    # print(fn)\n",
    "\n",
    "# 'read_hs_file' functionality:\n",
    "# loading the data files one by one through 'netCDF4' module, but in a random order of times\n",
    "data = []\n",
    "P = []\n",
    "timeo = []\n",
    "\n",
    "for i in range(len(fn)):\n",
    "\n",
    "    file = nc.Dataset(fn[i], 'r')\n",
    "\n",
    "    lat = file.variables['lat']  # Latitude \n",
    "    lon = file.variables['lon']  # Longitude\n",
    "    if read_p == True:\n",
    "        P = file.variables['lev'][:]  # Pressure levels\n",
    "\n",
    "    tt = (file.variables['time'])  # numeric value\n",
    "\n",
    "    # create a shape = (n, 3) array to store the (year, mon, day) cf.datetime object:\n",
    "    time_i = np.zeros((len(tt), 3))\n",
    "\n",
    "    for i in range(len(tt)):\n",
    "\n",
    "        tt1 = nc.num2date(tt[i], file.variables['time'].units,calendar = u'standard')  # cf.Datetime object: including yr, mon, day, hour, minute, second info\n",
    "\n",
    "        time_i[i,:] = [tt1.year, tt1.month, tt1.day]\n",
    "    # print(np.asarray(time_i).shape)\n",
    "\n",
    "    data_pieces = []\n",
    "\n",
    "    # determine whether the variable_time within the time_range we want:\n",
    "    if valid_range1[0] != valid_range2[0]:   # case 1, starting time and ending time are the different year.\n",
    "        if ((time_i[0, 0] > valid_range1[0]) & (time_i[0, 0] < valid_range2[0])) | ((time_i[0, 0] == valid_range1[0]) & (time_i[0, 1] >= valid_range1[1])) | ((time_i[0, 0] == valid_range2[0]) & (time_i[0, 1] <= valid_range2[1])):\n",
    "\n",
    "            data_pieces = file.variables[varnm]\n",
    "\n",
    "    elif (valid_range1[0] == valid_range2[0]) & (time_i[0, 0] == valid_range2[0]):   # case 2, starting and ending time are the same year.\n",
    "        if  (time_i[0, 1] >= valid_range1[1]) & (time_i[0, 1] <= valid_range2[1]):\n",
    "\n",
    "            data_pieces = file.variables[varnm]\n",
    "\n",
    "\n",
    "    # end 'read_hs_file' functionality.\n",
    "\n",
    "    if len(data_pieces) > 0:\n",
    "        data.append(data_pieces)  # Variable\n",
    "        timeo.append(time_i)  # Times\n",
    "# ending loop, and end 'read_hs' functionality.\n",
    "\n",
    "print(np.asarray(P).shape)\n",
    "\n",
    "\n",
    "# 'read_var_mod' functionality\n",
    "# processing lat, lon, P, data, and time array, output in an ordered arrangement\n",
    "\n",
    "# use 'np.concatenate' to get rid of one extra axis (the second axes)\n",
    "dataOUT = np.concatenate(data, axis = 0)\n",
    "timeOUT = np.concatenate(timeo, axis = 0)\n",
    "\n",
    "# replacing fill value to be 'np.nan'\n",
    "dataOUT = np.asarray(dataOUT)\n",
    "dataOUT[dataOUT == file.variables[varnm]._FillValue] = np.nan\n",
    "\n",
    "# use 'np.unique' to get ordered time and data array\n",
    "tf = timeOUT[:, 0] + timeOUT[:, 1]/100.\n",
    "TF, ind = np.unique(tf, return_index = True)  # TF is the sorted (time from smaller value to bigger value), unique 'tf', and ind is the indices\n",
    "\n",
    "dataOUT = dataOUT[ind]\n",
    "timeOUT = timeOUT[ind]\n",
    "\n",
    "# return np.ma.array(dataOUT, fill_value=np.nan), np.ma.array(lat[:], fill_value=np.nan), np.ma.array(lon[:], fill_value=np.nan), np.asarray(P), timeOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2cd3c6-c577-491d-a7ec-93a3ffc2baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---------------\n",
    "# This function is for reading MERRA-2 Re-analysis (Meteorology) dataset (netCDF-4 classic).\n",
    "# read_p = False: 2-D variable; True: 3-D variable (e.g., air Temerature, vetical Pressure velocity)\n",
    "# valid_range1 & 2: the starting and end time_stamps of time sequential metric. Only first two numbers are valid for monthly data.\n",
    "# data_type: there are two types of MERRA-2 data, '1': raw resolution (0.5 X 0.625, -180 ~ 180, -90 ~ 90)\n",
    "# '2': regrided resolution (1 X 1, 0.5 ~ 359.5, 89.5 ~ -89.5)\n",
    "### ---------------\n",
    "pp_path_OBS='/glade/scratch/chuyan/obs_data/'\n",
    "varnm = 'OMEGA500'\n",
    "read_p = False\n",
    "valid_range1 = [2002, 7, 15]\n",
    "valid_range2 = [2016, 12, 31]\n",
    "data_type= '2'\n",
    "# 'read_hs' functionality:\n",
    "# read the data file, according to the variable name: varnm and the data loading path: pp_path_OBS\n",
    "\n",
    "folder = pp_path_OBS\n",
    "if data_type == '1':\n",
    "    if varnm in ['EFLUX', 'PRECTOT']:\n",
    "        fn = glob.glob(folder + '*'+ 'tavgM_2d_flx_Nx.' + '*' + 'nc4.nc4?' + 'EFLUX,PRECTOT' + '*')\n",
    "        # print(fn)\n",
    "    elif varnm in ['OMEGA500', 'PS', 'QV10M', 'T2M', 'TS']:\n",
    "        fn = glob.glob(folder + '*'+ 'tavgM_2d_slv_Nx.' + '*' + 'nc4.nc4?' + 'OMEGA500,PS,QV10M,T2M,TS' + '*')\n",
    "        # print(fn)\n",
    "    elif varnm in ['OMEGA', 'T']:\n",
    "        fn = glob.glob(folder + '*'+ 'instM_3d_asm_Np.' + '*' + 'nc4.nc4?' + 'OMEGA,T' + '*')\n",
    "        # print(fn)\n",
    "\n",
    "if data_type == '2':\n",
    "    if varnm in ['EFLUX', 'PRECTOT']:\n",
    "        fn = glob.glob(folder + '*'+ 'tavgM_2d_flx_Nx.' + '*' + '.nc')\n",
    "        print(fn)\n",
    "    elif varnm in ['OMEGA500', 'PS', 'QV10M', 'T2M', 'TS']:\n",
    "        fn = glob.glob(folder + '*'+ 'tavgM_2d_slv_Nx.' + '*' + '.nc')\n",
    "        print(fn)\n",
    "    elif varnm in ['OMEGA', 'T']:\n",
    "        fn = glob.glob(folder + '*'+ 'instM_3d_asm_Np.' + '*' + '.nc')\n",
    "        print(fn)\n",
    "    \n",
    "# 'read_hs_file' functionality:\n",
    "# loading the data files one by one through 'netCDF4' module, but in a random order of times\n",
    "data = []\n",
    "P = []\n",
    "timeo = []\n",
    "\n",
    "for i in range(len(fn)):\n",
    "\n",
    "    file = nc.Dataset(fn[i], 'r')  # random order of times (due to the functionality of \"glob\")\n",
    "\n",
    "    lat = file.variables['lat']  # Latitude \n",
    "    lon = file.variables['lon']  # Longitude\n",
    "    if read_p == True:\n",
    "        P = file.variables['lev'][:]  # Pressure levels\n",
    "\n",
    "    tt = (file.variables['time'])  # numeric value\n",
    "\n",
    "    # create a shape = (n, 3) array to store the (year, mon, day) cf.datetime object:\n",
    "    time_i = np.zeros((len(tt), 3))\n",
    "\n",
    "    for i in range(len(tt)):\n",
    "\n",
    "        tt1 = nc.num2date(tt[i], file.variables['time'].units,calendar = u'standard')  # cf.Datetime object: including yr, mon, day, hour, minute, second info\n",
    "\n",
    "        time_i[i,:] = [tt1.year, tt1.month, tt1.day]\n",
    "    # print(np.asarray(time_i).shape)\n",
    "\n",
    "    data_pieces = []\n",
    "\n",
    "    # determine whether the variable_time within the time_range we want:\n",
    "    if valid_range1[0] != valid_range2[0]:   # case 1, starting time and ending time are the different year.\n",
    "        if ((time_i[0, 0] > valid_range1[0]) & (time_i[0, 0] < valid_range2[0])) | ((time_i[0, 0] == valid_range1[0]) & (time_i[0, 1] >= valid_range1[1])) | ((time_i[0, 0] == valid_range2[0]) & (time_i[0, 1] <= valid_range2[1])):\n",
    "\n",
    "            data_pieces = file.variables[varnm]\n",
    "\n",
    "    elif (valid_range1[0] == valid_range2[0]) & (time_i[0, 0] == valid_range2[0]):   # case 2, starting and ending time are the same year.\n",
    "        if  (time_i[0, 1] >= valid_range1[1]) & (time_i[0, 1] <= valid_range2[1]):\n",
    "\n",
    "            data_pieces = file.variables[varnm]\n",
    "\n",
    "\n",
    "    # end 'read_hs_file' functionality.\n",
    "\n",
    "    if len(data_pieces) > 0:\n",
    "        data.append(data_pieces)  # Variable\n",
    "        timeo.append(time_i)  # Times\n",
    "# ending loop, and end 'read_hs' functionality.\n",
    "\n",
    "print(np.asarray(P).shape)\n",
    "\n",
    "\n",
    "# 'read_var_mod' functionality\n",
    "# processing lat, lon, P, data, and time array, output in an ordered arrangement\n",
    "\n",
    "# use 'np.concatenate' to get rid of one extra axis (the second axes)\n",
    "dataOUT = np.concatenate(data, axis = 0)\n",
    "timeOUT = np.concatenate(timeo, axis = 0)\n",
    "\n",
    "# replacing fill value to be 'np.nan'\n",
    "dataOUT = np.asarray(dataOUT)\n",
    "dataOUT[dataOUT == file.variables[varnm]._FillValue] = np.nan\n",
    "\n",
    "# use 'np.unique' to get ordered time and data array\n",
    "tf = timeOUT[:, 0] + timeOUT[:, 1]/100.\n",
    "TF, ind = np.unique(tf, return_index = True)  # TF is the sorted (time from smaller value to bigger value), unique 'tf', and ind is the indices\n",
    "\n",
    "dataOUT = dataOUT[ind]\n",
    "timeOUT = timeOUT[ind]\n",
    "\n",
    "# return np.ma.array(dataOUT, fill_value=np.nan), np.ma.array(lat[:], fill_value=np.nan), np.ma.array(lon[:], fill_value=np.nan), np.asarray(P), timeOUT\n",
    "\n",
    "if data_type == '2':\n",
    "\n",
    "    lat = lat[::-1]\n",
    "    \n",
    "    lon2 = lon[:] *1.\n",
    "    lon2[lon2 > 180] = lon2[lon2 > 180] - 360.  # convert to range from -180 to 180.\n",
    "    ind_sort = np.argsort(lon2)\n",
    "    lon2 = lon2[ind_sort]\n",
    "    lon = lon2\n",
    "    dataOUT1 = dataOUT.copy()\n",
    "    if read_p == True:\n",
    "        dataOUT1 = dataOUT1[:, :, ::-1, :]\n",
    "        dataOUT1 = dataOUT1[:, :, :, ind_sort]\n",
    "\n",
    "    else:\n",
    "        dataOUT1 = dataOUT1[:, ::-1, :]\n",
    "        dataOUT1 = dataOUT1[:, :, ind_sort]\n",
    "\n",
    "        dataOUT = dataOUT1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47508355-2c59-4468-9c56-31e69b008bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9b9eca3-8077-4bef-8db4-f10d3b5b03a8",
   "metadata": {},
   "source": [
    "## read_var_obs_MAClwp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f390f78c-2b48-41a0-8b87-e6e8f48fe25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAC-LWP\n",
    "from netCDF4 import MFDataset \n",
    "\n",
    "path1 = '/glade/scratch/chuyan/obs_data/'\n",
    "import glob as glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28c8ffb9-23ec-4c8d-a826-eb738ced5cb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a50fed9d9df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMFDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"maclwp_cloudlwpave_*_v1.nc4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.MFDataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: MFNetCDF4 only works with NETCDF3_* and NETCDF4_CLASSIC formatted files, not NETCDF4"
     ]
    }
   ],
   "source": [
    "f = MFDataset(path1+\"maclwp_cloudlwpave_*_v1.nc4\")\n",
    "print(f.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f13527cf-024f-4bd1-8724-fbc786123477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\n",
      " 2016]\n"
     ]
    }
   ],
   "source": [
    "folder = path1\n",
    "valid_range1 = [2002, 7, 15]\n",
    "valid_range2 = [2016, 12, 31]\n",
    "\n",
    "varnm = 'cloudlwp'\n",
    "### ---------------\n",
    "\n",
    "folder = pp_path_OBS\n",
    "\n",
    "fn = glob.glob(folder+\"maclwp_cloudlwpave_*_v1.nc4\")\n",
    "# print(fn)\n",
    "tt_str = np.arange(valid_range1[0], valid_range2[0]+1, 1)\n",
    "print(tt_str)\n",
    "\n",
    "data = []  # Variable\n",
    "timeo = []  # Times\n",
    "data_error = []  # Statistic 1-Sigma Error\n",
    "\n",
    "for F in range(0, (valid_range2[0] - valid_range1[0] + 1)):\n",
    "\n",
    "    f = nc.Dataset(folder + \"maclwp_cloudlwpave_\" + str(tt_str[F]) + \"_v1.nc4\")\n",
    "\n",
    "\n",
    "    # read data_slice, time_slice, lat, lon.\n",
    "    lat = f.variables['lat']   # Latitude\n",
    "    lat_bnds = f.variables['lat_bnds']  # latitude bounds, in shape (180, 2)\n",
    "    lon = f.variables['lon']   # Longitude\n",
    "    lon_bnds = f.variables['lon_bnds']  # longitude bounds, in shape (360, 2)\n",
    "    time_slice = f.variables['time']  # numeric value time for 12 months in a year\n",
    "\n",
    "\n",
    "    # create a shape = (n, 3) array to store the (year, month, day) cf.datetime object\n",
    "    time_i = np.zeros((len(time_slice), 3))\n",
    "    for i in range(len(time_slice)):\n",
    "\n",
    "        tt1 = nc.num2date(time_slice[i], f.variables['time'].units,calendar = u'360_day')  # cf.Datetime object: including yr, mon, day, hour, minute, second info\n",
    "        # print(tt1)\n",
    "        time_i[i, :] = [tt1.year, tt1.month, tt1.day]\n",
    "    # print(time_i)\n",
    "\n",
    "    # determine whether the variable_time within the time_range we want:\n",
    "    if valid_range1[0] != valid_range2[0]:   # case 1, starting time and ending time are the different year.\n",
    "        if (time_i[0, 0] <= valid_range2[0]) & (time_i[0, 0] >= valid_range1[0]):\n",
    "\n",
    "            if ((time_i[0,0] == valid_range1[0]) & (valid_range1[1] > 1)):\n",
    "                data_slice = f.variables[varnm][(valid_range1[1]-1):]  # data in a year, shape in (12, 180, 360)\n",
    "                data_errorslice = f.variables[varnm+'_error'][(valid_range1[1]-1):]  # statistic data error, shape also in (12, 180, 360)\n",
    "\n",
    "                time_i = time_i[(valid_range1[1]-1):] *1.\n",
    "            elif ((time_i[0,0] == valid_range2[0]) & (valid_range2[1] < 12)):\n",
    "                data_slice = f.variables[varnm][:(valid_range2[1])]\n",
    "                data_errorslice = f.variables[varnm+'_error'][:(valid_range2[1])]\n",
    "\n",
    "                time_i = time_i[:(valid_range2[1])] *1.\n",
    "            else:\n",
    "                data_slice = np.ma.array(f.variables[varnm])  # ..\n",
    "                data_errorslice = f.variables[varnm+'_error']\n",
    "\n",
    "                time_i = time_i *1.\n",
    "\n",
    "    elif (valid_range1[0] == valid_range2[0]) & (time_i[0, 0] == valid_range2[0]):   # case 2, starting and ending time are the same year.\n",
    "\n",
    "        data_slice = f.variables[varnm][(valid_range1[1]-1):(valid_range2[1])]\n",
    "        data_errorslice = f.variables[varnm+'_error'][(valid_range1[1]-1):(valid_range2[1])]\n",
    "\n",
    "        time_i = time_i[(valid_range1[1]-1):(valid_range2[1])] *1.\n",
    "\n",
    "    # concatenate the data_slice and time_slice:\n",
    "    if len(data_slice) > 0:\n",
    "\n",
    "        data.append(data_slice)\n",
    "        timeo.append(time_i)\n",
    "        data_error.append(data_errorslice)\n",
    "    \n",
    "# use 'np.concatenate' to get rid of one extra axis \n",
    "dataOUT = np.ma.concatenate(data, axis = 0)\n",
    "timeOUT = np.concatenate(timeo, axis = 0)\n",
    "data_errorOUT = np.ma.concatenate(data_error, axis = 0)\n",
    "\n",
    "# retrieve the \"filled_value\" and the mask of \"MaskedArray\" \n",
    "# and replace the MaskedArray as an ndarray, with mask position as 'np.nan':\n",
    "filled_value_mac = f.variables[varnm]._FillValue\n",
    "Mask_arrayOUT = np.ma.getmaskarray(dataOUT)\n",
    "\n",
    "dataOUT2 = dataOUT.data  # the data array (ndarray), with \"filled_value_mac\" in the masked position instead of '-'\n",
    "data_errorOUT2 = data_errorOUT.data\n",
    "\n",
    "dataOUT2[dataOUT2 == filled_value_mac] = np.nan\n",
    "data_errorOUT2[data_errorOUT2 == filled_value_mac] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f7bdd48-d492-4781-8457-ff1626ae5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True False False False False  True  True\n",
      "  True  True  True  True  True  True False False False False  True  True\n",
      "  True  True  True  True  True  True False False False False  True  True\n",
      "  True  True  True  True  True  True False False False False  True  True\n",
      "  True  True  True  True  True  True False False False False  True  True\n",
      "  True  True  True  True  True  True False False False False  True  True\n",
      "  True  True  True  True  True  True  True False False  True  True  True\n",
      "  True  True  True  True  True  True False False False False  True  True\n",
      "  True  True  True  True  True  True False False False False  True  True\n",
      "  True  True  True  True  True  True  True False False  True  True  True\n",
      "  True  True  True  True  True  True False False False False  True  True\n",
      "  True  True  True  True  True  True False False False False  True  True\n",
      "  True  True  True  True  True  True False False False False  True  True\n",
      "  True  True  True  True  True  True False False False False  True  True\n",
      "  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(np.ma.getmaskarray(dataOUT)[:, 24, 354])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd766707-e7b0-4c6b-b802-e2afe45ca763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.390526\n",
      "4.07791\n",
      "0.4918932701858947\n"
     ]
    }
   ],
   "source": [
    "print(dataOUT2[6, 24, 354])\n",
    "print(data_errorOUT2[6, 24, 354])\n",
    "\n",
    "print(np.asarray(np.nonzero(np.ma.getmaskarray(dataOUT) == True)).shape[1] / (174* 180*360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd4c1e-58ba-480a-a6bf-9c434792f9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "575226e9-e16b-4274-8868-9feb47c87d3d",
   "metadata": {},
   "source": [
    "## read_var_obs_CERES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d4e17f-58e4-4487-8e67-be76a36bc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/glade/scratch/chuyan/obs_data/'\n",
    "import netCDF4 as nc\n",
    "import glob as glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ee4efff-2a4d-4fb7-8ca4-785ef6ffb90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lon': <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 lon(lon)\n",
      "    long_name: Longitude\n",
      "    standard_name: longitude\n",
      "    units: degrees_east\n",
      "    valid_range: [-180.  360.]\n",
      "unlimited dimensions: \n",
      "current shape = (360,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used, 'lat': <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 lat(lat)\n",
      "    long_name: Latitude\n",
      "    standard_name: latitude\n",
      "    units: degrees_north\n",
      "    valid_range: [-90.  90.]\n",
      "unlimited dimensions: \n",
      "current shape = (45,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used, 'time': <class 'netCDF4._netCDF4.Variable'>\n",
      "int32 time(time)\n",
      "    long_name: Time\n",
      "    units: days since 2000-03-01 00:00:00\n",
      "    delta_t: 0000-00-01 00:00:00\n",
      "unlimited dimensions: \n",
      "current shape = (237,)\n",
      "filling on, default _FillValue of -2147483647 used, 'toa_sw_all_mon': <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 toa_sw_all_mon(time, lat, lon)\n",
      "    long_name: Top of The Atmosphere Shortwave Flux, All-Sky conditions, Monthly Means\n",
      "    standard_name: TOA Shortwave Flux - All-Sky\n",
      "    CF_name: toa_outgoing_shortwave_flux\n",
      "    comment: none\n",
      "    units: W m-2\n",
      "    valid_min:       0.00000\n",
      "    valid_max:       600.000\n",
      "    _FillValue: -999.0\n",
      "unlimited dimensions: \n",
      "current shape = (237, 45, 360)\n",
      "filling on, 'toa_lw_all_mon': <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 toa_lw_all_mon(time, lat, lon)\n",
      "    long_name: Top of The Atmosphere Longwave Flux, All-Sky conditions, Monthly Means\n",
      "    standard_name: TOA Longwave Flux - All-Sky\n",
      "    CF_name: toa_outgoing_longwave_flux\n",
      "    comment: none\n",
      "    units: W m-2\n",
      "    valid_min:       0.00000\n",
      "    valid_max:       400.000\n",
      "    _FillValue: -999.0\n",
      "unlimited dimensions: \n",
      "current shape = (237, 45, 360)\n",
      "filling on, 'toa_net_all_mon': <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 toa_net_all_mon(time, lat, lon)\n",
      "    long_name: Top of The Atmosphere Net Flux, All-Sky conditions, Monthly Means\n",
      "    standard_name: TOA Net Flux - All-Sky\n",
      "    CF_name: toa_net_downward_flux\n",
      "    comment: none\n",
      "    units: W m-2\n",
      "    valid_min:      -400.000\n",
      "    valid_max:       400.000\n",
      "    _FillValue: -999.0\n",
      "unlimited dimensions: \n",
      "current shape = (237, 45, 360)\n",
      "filling on, 'toa_sw_clr_c_mon': <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 toa_sw_clr_c_mon(time, lat, lon)\n",
      "    long_name: Top of The Atmosphere Shortwave Flux, Clear-Sky (for cloud-free areas of region) conditions, Monthly Means\n",
      "    standard_name: TOA Shortwave Flux - Clear-Sky (for cloud-free areas of region)\n",
      "    CF_name: none\n",
      "    comment: none\n",
      "    units: W m-2\n",
      "    valid_min:       0.00000\n",
      "    valid_max:       600.000\n",
      "    _FillValue: -999.0\n",
      "unlimited dimensions: \n",
      "current shape = (237, 45, 360)\n",
      "filling on, 'toa_lw_clr_c_mon': <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 toa_lw_clr_c_mon(time, lat, lon)\n",
      "    long_name: Top of The Atmosphere Longwave Flux, Clear-Sky (for cloud-free areas of region) conditions, Monthly Means\n",
      "    standard_name: TOA Longwave Flux - Clear-Sky (for cloud-free areas of region)\n",
      "    CF_name: none\n",
      "    comment: none\n",
      "    units: W m-2\n",
      "    valid_min:       0.00000\n",
      "    valid_max:       400.000\n",
      "    _FillValue: -999.0\n",
      "unlimited dimensions: \n",
      "current shape = (237, 45, 360)\n",
      "filling on, 'toa_net_clr_c_mon': <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 toa_net_clr_c_mon(time, lat, lon)\n",
      "    long_name: Top of The Atmosphere Net Flux, Clear-Sky (for cloud-free areas of region) conditions, Monthly Means\n",
      "    standard_name: TOA Net Flux - Clear-Sky (for cloud-free areas of region)\n",
      "    CF_name: none\n",
      "    comment: none\n",
      "    units: W m-2\n",
      "    valid_min:      -400.000\n",
      "    valid_max:       400.000\n",
      "    _FillValue: -999.0\n",
      "unlimited dimensions: \n",
      "current shape = (237, 45, 360)\n",
      "filling on, 'solar_mon': <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 solar_mon(time, lat, lon)\n",
      "    long_name: Incoming Solar Flux, Monthly Means\n",
      "    standard_name: Incoming Solar Flux\n",
      "    CF_name: toa_incoming_shortwave_flux\n",
      "    comment: none\n",
      "    units: W m-2\n",
      "    valid_min:       0.00000\n",
      "    valid_max:       800.000\n",
      "    _FillValue: -999.0\n",
      "unlimited dimensions: \n",
      "current shape = (237, 45, 360)\n",
      "filling on, 'cldarea_total_daynight_mon': <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 cldarea_total_daynight_mon(time, lat, lon)\n",
      "    long_name: Cloud Area Fraction, Daytime-and-Nighttime conditions, Monthly Means\n",
      "    standard_name: Cloud Area Fraction - Daytime-and-Nighttime\n",
      "    CF_name: cloud_area_fraction\n",
      "    comment: none\n",
      "    units: percent\n",
      "    valid_min:       0.00000\n",
      "    valid_max:       100.000\n",
      "    _FillValue: -999.0\n",
      "unlimited dimensions: \n",
      "current shape = (237, 45, 360)\n",
      "filling on, 'cldpress_total_daynight_mon': <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 cldpress_total_daynight_mon(time, lat, lon)\n",
      "    long_name: Cloud Effective Pressure, Daytime-and-Nighttime conditions, Monthly Means\n",
      "    standard_name: Cloud Effective Pressure - Daytime-and-Nighttime\n",
      "    CF_name: none\n",
      "    comment: none\n",
      "    units: hPa\n",
      "    valid_min:       0.00000\n",
      "    valid_max:       1050.00\n",
      "    _FillValue: -999.0\n",
      "unlimited dimensions: \n",
      "current shape = (237, 45, 360)\n",
      "filling on, 'cldtau_total_day_mon': <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 cldtau_total_day_mon(time, lat, lon)\n",
      "    long_name: Cloud Visible Optical Depth, Daytime conditions, Monthly Means\n",
      "    standard_name: Cloud Visible Optical Depth - Daytime\n",
      "    CF_name: none\n",
      "    comment: none\n",
      "    units: dimensionless\n",
      "    valid_min:       0.00000\n",
      "    valid_max:       250.000\n",
      "    _FillValue: -999.0\n",
      "unlimited dimensions: \n",
      "current shape = (237, 45, 360)\n",
      "filling on}\n",
      "-999.0\n",
      "(237, 45, 360)\n",
      "(3, 0)\n",
      "[ 866  897  928  958  989 1019 1050 1081 1109 1140 1170 1201 1231 1262\n",
      " 1293 1323 1354 1384 1415 1446 1475 1506 1536 1567 1597 1628 1659 1689\n",
      " 1720 1750 1781 1812 1840 1871 1901 1932 1962 1993 2024 2054 2085 2115\n",
      " 2146 2177 2205 2236 2266 2297 2327 2358 2389 2419 2450 2480 2511 2542\n",
      " 2570 2601 2631 2662 2692 2723 2754 2784 2815 2845 2876 2907 2936 2967\n",
      " 2997 3028 3058 3089 3120 3150 3181 3211 3242 3273 3301 3332 3362 3393\n",
      " 3423 3454 3485 3515 3546 3576 3607 3638 3666 3697 3727 3758 3788 3819\n",
      " 3850 3880 3911 3941 3972 4003 4031 4062 4092 4123 4153 4184 4215 4245\n",
      " 4276 4306 4337 4368 4397 4428 4458 4489 4519 4550 4581 4611 4642 4672\n",
      " 4703 4734 4762 4793 4823 4854 4884 4915 4946 4976 5007 5037 5068 5099\n",
      " 5127 5158 5188 5219 5249 5280 5311 5341 5372 5402 5433 5464 5492 5523\n",
      " 5553 5584 5614 5645 5676 5706 5737 5767 5798 5829 5858 5889 5919 5950\n",
      " 5980 6011 6042 6072 6103 6133 6164 6195 6223 6254 6284 6315 6345 6376\n",
      " 6407 6437 6468 6498 6529 6560 6588 6619 6649 6680 6710 6741 6772 6802\n",
      " 6833 6863 6894 6925 6953 6984 7014 7045 7075 7106 7137 7167 7198 7228\n",
      " 7259 7290 7319 7350 7380 7411 7441 7472 7503 7533 7564 7594 7625 7656\n",
      " 7684 7715 7745 7776 7806 7837 7868 7898 7929 7959 7990 8021 8049]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/ssg/ch/usr/jupyterhub/envs/npl-3.7.9/dav/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: WARNING: valid_min not used since it\n",
      "cannot be safely cast to variable data type\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/glade/u/ssg/ch/usr/jupyterhub/envs/npl-3.7.9/dav/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: WARNING: valid_max not used since it\n",
      "cannot be safely cast to variable data type\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/glade/u/ssg/ch/usr/jupyterhub/envs/npl-3.7.9/dav/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: WARNING: valid_min not used since it\n",
      "cannot be safely cast to variable data type\n",
      "  del sys.path[0]\n",
      "/glade/u/ssg/ch/usr/jupyterhub/envs/npl-3.7.9/dav/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: WARNING: valid_max not used since it\n",
      "cannot be safely cast to variable data type\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# --------\n",
    "folder = path1\n",
    "\n",
    "fn = glob.glob(folder+\"CERES_EBAF-TOA_Ed4.1_Subset_200207-202203.nc\")\n",
    "\n",
    "# print(fn)\n",
    "print(nc.Dataset(fn[0], 'r').variables)\n",
    "data = nc.Dataset(fn[0], 'r').variables['toa_sw_all_mon']\n",
    "Fill_value = nc.Dataset(fn[0], 'r').variables['toa_sw_all_mon']._FillValue\n",
    "ind_filled = data == Fill_value\n",
    "print(Fill_value)\n",
    "# print(ind_filled)\n",
    "print(np.asarray(data[:]).shape)\n",
    "print(np.asarray(np.nonzero(ind_filled == True)).shape)\n",
    "print(np.asarray(nc.Dataset(fn[0], 'r').variables['time'][:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f05f0f4-119a-48a0-b374-b41c62f74247",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_slice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-33f5abde1516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create a shape = (n, 3) array to store the (year, month, day) cf.datetime object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtime_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum2date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_slice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcalendar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mu'360_day'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# cf.Datetime object: including yr, mon, day, hour, minute, second info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time_slice' is not defined"
     ]
    }
   ],
   "source": [
    "# create a shape = (n, 3) array to store the (year, month, day) cf.datetime object\n",
    "time_i = np.zeros((len(time_slice), 3))\n",
    "for i in range(len(time_slice)):\n",
    "\n",
    "    tt1 = nc.num2date(time_slice[i], f.variables['time'].units,calendar = u'360_day')  # cf.Datetime object: including yr, mon, day, hour, minute, second info\n",
    "    # print(tt1)\n",
    "    time_i[i, :] = [tt1.year, tt1.month, tt1.day]\n",
    "# print(time_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa5492f4-8ea3-41fd-8920-aa508b81ffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill Value:  -999.0\n",
      "(174, 45, 360)\n",
      "Fill Value:  -999.0\n",
      "(174, 45, 360)\n",
      "Fill Value:  -999.0\n",
      "(174, 45, 360)\n"
     ]
    }
   ],
   "source": [
    "rsut, lat_ceres, lon_ceres, time_ceres = read_var_obs_CERES(varnm = 'toa_sw_all_mon', valid_range1=[2002, 7, 15], valid_range2=[2016, 12, 31])\n",
    "rsdt, lat_ceres2, lon_ceres2, time_ceres2 = read_var_obs_CERES(varnm = 'solar_mon', valid_range1=[2002, 7, 15], valid_range2=[2016, 12, 31])\n",
    "rsutcs, lat_ceres3, lon_ceres3, time_ceres3 = read_var_obs_CERES(varnm = 'toa_sw_clr_c_mon', valid_range1=[2002, 7, 15], valid_range2=[2016, 12, 31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5bd60ca5-1fac-4a83-8b21-de37f17939fe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.500e-02 4.300e-02 4.100e-02 4.000e-02 4.100e-02 4.300e-02 4.600e-02\n",
      " 5.200e-02 6.100e-02 7.600e-02 1.030e-01 1.570e-01 2.590e-01 4.228e-01\n",
      " 5.402e-01 6.444e-01 7.220e-01 3.382e+00 4.818e+00 8.321e+00 1.044e+01\n",
      " 1.199e+01 1.473e+01 1.702e+01 1.781e+01 1.968e+01 2.222e+01 2.434e+01\n",
      " 2.619e+01 2.898e+01 3.006e+01 3.177e+01 3.381e+01 3.709e+01 3.894e+01\n",
      " 4.063e+01 4.197e+01 4.292e+01 4.406e+01 4.491e+01 4.485e+01 4.650e+01\n",
      " 4.961e+01 5.581e+01 5.844e+01]\n",
      "[     nan      nan      nan      nan      nan      nan      nan      nan\n",
      "      nan      nan      nan      nan      nan      nan   0.1815   0.762\n",
      "   1.868    3.661    6.357    9.866   13.9     18.35    23.12    28.16\n",
      "  33.44    38.91    44.55    50.35    56.28    62.32    68.46    74.69\n",
      "  81.      87.37    93.81   100.3    106.8    113.4    120.     126.6\n",
      " 133.2    139.9    146.5    153.2    159.9   ]\n",
      "[1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03 1.000e-03\n",
      " 1.000e-03 1.000e-03 1.000e-03 2.000e-03 2.000e-03 4.000e-03 3.108e-03\n",
      " 3.179e-03 3.176e-03 3.171e-03 4.084e-03 4.084e-03 8.143e+00 1.226e+01\n",
      " 1.211e+01 1.211e+01 1.212e+01 1.213e+01 1.172e+01 1.266e+01 1.269e+01\n",
      " 1.428e+01 1.558e+01 1.680e+01 1.765e+01 1.923e+01 2.084e+01 2.205e+01\n",
      " 2.307e+01 2.402e+01 2.483e+01 2.549e+01 2.585e+01 2.520e+01 2.490e+01\n",
      " 2.462e+01 2.453e+01 2.551e+01]\n",
      "[       nan        nan        nan        nan 5.554328   1.058019\n",
      " 0.44324327 1.1316856  0.9513722  0.8718878  0.81386375 0.79490703\n",
      " 0.7521525  0.7206166  0.7224379  0.71947294 0.71342963 0.70164704\n",
      " 0.6898473  0.65768737 0.58151007 0.5326379  0.5159183  0.5094411\n",
      " 0.49959534 0.47066408 0.4669409  0.45362183 0.45309734 0.4444816\n",
      " 0.4287411  0.41218045 0.41775233 0.40573773 0.3951634  0.40407014\n",
      " 0.3977751  0.39392716 0.38011143 0.36258066 0.3491173  0.35246232\n",
      " 0.34754258 0.32501182 0.32584783]\n",
      "[          nan           nan           nan           nan 3.9030079e-02\n",
      " 6.5810191e-03 2.5206676e-03 1.8444085e+00 2.7876747e+00 2.5716987e+00\n",
      " 1.7676879e+00 1.2700081e+00 6.4514142e-01 9.4605005e-01 8.9557451e-01\n",
      " 8.6407453e-01 8.4173250e-01 8.1458819e-01 7.5711924e-01 7.2219169e-01\n",
      " 6.1506826e-01 4.3666169e-01 2.5537416e-01 2.4234086e-01 2.3540296e-01\n",
      " 2.2286697e-01 2.1972509e-01 2.1966134e-01 2.0353982e-01 1.9774248e-01\n",
      " 1.9659540e-01 1.9105263e-01 1.8639944e-01 1.8545082e-01 1.7784314e-01\n",
      " 1.7626800e-01 1.7751053e-01 1.8120301e-01 1.7754875e-01 1.7037635e-01\n",
      " 1.5711318e-01 1.4688443e-01 1.4238442e-01 1.4436056e-01 1.3959670e-01]\n",
      "[[2.002e+03 7.000e+00 1.500e+01]\n",
      " [2.002e+03 8.000e+00 1.500e+01]\n",
      " [2.002e+03 9.000e+00 1.500e+01]\n",
      " [2.002e+03 1.000e+01 1.500e+01]\n",
      " [2.002e+03 1.100e+01 1.500e+01]\n",
      " [2.002e+03 1.200e+01 1.500e+01]\n",
      " [2.003e+03 1.000e+00 1.500e+01]\n",
      " [2.003e+03 2.000e+00 1.500e+01]\n",
      " [2.003e+03 3.000e+00 1.500e+01]\n",
      " [2.003e+03 4.000e+00 1.500e+01]\n",
      " [2.003e+03 5.000e+00 1.500e+01]\n",
      " [2.003e+03 6.000e+00 1.500e+01]\n",
      " [2.003e+03 7.000e+00 1.500e+01]\n",
      " [2.003e+03 8.000e+00 1.500e+01]\n",
      " [2.003e+03 9.000e+00 1.500e+01]\n",
      " [2.003e+03 1.000e+01 1.500e+01]\n",
      " [2.003e+03 1.100e+01 1.500e+01]\n",
      " [2.003e+03 1.200e+01 1.500e+01]\n",
      " [2.004e+03 1.000e+00 1.500e+01]\n",
      " [2.004e+03 2.000e+00 1.500e+01]\n",
      " [2.004e+03 3.000e+00 1.500e+01]\n",
      " [2.004e+03 4.000e+00 1.500e+01]\n",
      " [2.004e+03 5.000e+00 1.500e+01]\n",
      " [2.004e+03 6.000e+00 1.500e+01]\n",
      " [2.004e+03 7.000e+00 1.500e+01]\n",
      " [2.004e+03 8.000e+00 1.500e+01]\n",
      " [2.004e+03 9.000e+00 1.500e+01]\n",
      " [2.004e+03 1.000e+01 1.500e+01]\n",
      " [2.004e+03 1.100e+01 1.500e+01]\n",
      " [2.004e+03 1.200e+01 1.500e+01]\n",
      " [2.005e+03 1.000e+00 1.500e+01]\n",
      " [2.005e+03 2.000e+00 1.500e+01]\n",
      " [2.005e+03 3.000e+00 1.500e+01]\n",
      " [2.005e+03 4.000e+00 1.500e+01]\n",
      " [2.005e+03 5.000e+00 1.500e+01]\n",
      " [2.005e+03 6.000e+00 1.500e+01]\n",
      " [2.005e+03 7.000e+00 1.500e+01]\n",
      " [2.005e+03 8.000e+00 1.500e+01]\n",
      " [2.005e+03 9.000e+00 1.500e+01]\n",
      " [2.005e+03 1.000e+01 1.500e+01]\n",
      " [2.005e+03 1.100e+01 1.500e+01]\n",
      " [2.005e+03 1.200e+01 1.500e+01]\n",
      " [2.006e+03 1.000e+00 1.500e+01]\n",
      " [2.006e+03 2.000e+00 1.500e+01]\n",
      " [2.006e+03 3.000e+00 1.500e+01]\n",
      " [2.006e+03 4.000e+00 1.500e+01]\n",
      " [2.006e+03 5.000e+00 1.500e+01]\n",
      " [2.006e+03 6.000e+00 1.500e+01]\n",
      " [2.006e+03 7.000e+00 1.500e+01]\n",
      " [2.006e+03 8.000e+00 1.500e+01]\n",
      " [2.006e+03 9.000e+00 1.500e+01]\n",
      " [2.006e+03 1.000e+01 1.500e+01]\n",
      " [2.006e+03 1.100e+01 1.500e+01]\n",
      " [2.006e+03 1.200e+01 1.500e+01]\n",
      " [2.007e+03 1.000e+00 1.500e+01]\n",
      " [2.007e+03 2.000e+00 1.500e+01]\n",
      " [2.007e+03 3.000e+00 1.500e+01]\n",
      " [2.007e+03 4.000e+00 1.500e+01]\n",
      " [2.007e+03 5.000e+00 1.500e+01]\n",
      " [2.007e+03 6.000e+00 1.500e+01]\n",
      " [2.007e+03 7.000e+00 1.500e+01]\n",
      " [2.007e+03 8.000e+00 1.500e+01]\n",
      " [2.007e+03 9.000e+00 1.500e+01]\n",
      " [2.007e+03 1.000e+01 1.500e+01]\n",
      " [2.007e+03 1.100e+01 1.500e+01]\n",
      " [2.007e+03 1.200e+01 1.500e+01]\n",
      " [2.008e+03 1.000e+00 1.500e+01]\n",
      " [2.008e+03 2.000e+00 1.500e+01]\n",
      " [2.008e+03 3.000e+00 1.500e+01]\n",
      " [2.008e+03 4.000e+00 1.500e+01]\n",
      " [2.008e+03 5.000e+00 1.500e+01]\n",
      " [2.008e+03 6.000e+00 1.500e+01]\n",
      " [2.008e+03 7.000e+00 1.500e+01]\n",
      " [2.008e+03 8.000e+00 1.500e+01]\n",
      " [2.008e+03 9.000e+00 1.500e+01]\n",
      " [2.008e+03 1.000e+01 1.500e+01]\n",
      " [2.008e+03 1.100e+01 1.500e+01]\n",
      " [2.008e+03 1.200e+01 1.500e+01]\n",
      " [2.009e+03 1.000e+00 1.500e+01]\n",
      " [2.009e+03 2.000e+00 1.500e+01]\n",
      " [2.009e+03 3.000e+00 1.500e+01]\n",
      " [2.009e+03 4.000e+00 1.500e+01]\n",
      " [2.009e+03 5.000e+00 1.500e+01]\n",
      " [2.009e+03 6.000e+00 1.500e+01]\n",
      " [2.009e+03 7.000e+00 1.500e+01]\n",
      " [2.009e+03 8.000e+00 1.500e+01]\n",
      " [2.009e+03 9.000e+00 1.500e+01]\n",
      " [2.009e+03 1.000e+01 1.500e+01]\n",
      " [2.009e+03 1.100e+01 1.500e+01]\n",
      " [2.009e+03 1.200e+01 1.500e+01]\n",
      " [2.010e+03 1.000e+00 1.500e+01]\n",
      " [2.010e+03 2.000e+00 1.500e+01]\n",
      " [2.010e+03 3.000e+00 1.500e+01]\n",
      " [2.010e+03 4.000e+00 1.500e+01]\n",
      " [2.010e+03 5.000e+00 1.500e+01]\n",
      " [2.010e+03 6.000e+00 1.500e+01]\n",
      " [2.010e+03 7.000e+00 1.500e+01]\n",
      " [2.010e+03 8.000e+00 1.500e+01]\n",
      " [2.010e+03 9.000e+00 1.500e+01]\n",
      " [2.010e+03 1.000e+01 1.500e+01]\n",
      " [2.010e+03 1.100e+01 1.500e+01]\n",
      " [2.010e+03 1.200e+01 1.500e+01]\n",
      " [2.011e+03 1.000e+00 1.500e+01]\n",
      " [2.011e+03 2.000e+00 1.500e+01]\n",
      " [2.011e+03 3.000e+00 1.500e+01]\n",
      " [2.011e+03 4.000e+00 1.500e+01]\n",
      " [2.011e+03 5.000e+00 1.500e+01]\n",
      " [2.011e+03 6.000e+00 1.500e+01]\n",
      " [2.011e+03 7.000e+00 1.500e+01]\n",
      " [2.011e+03 8.000e+00 1.500e+01]\n",
      " [2.011e+03 9.000e+00 1.500e+01]\n",
      " [2.011e+03 1.000e+01 1.500e+01]\n",
      " [2.011e+03 1.100e+01 1.500e+01]\n",
      " [2.011e+03 1.200e+01 1.500e+01]\n",
      " [2.012e+03 1.000e+00 1.500e+01]\n",
      " [2.012e+03 2.000e+00 1.500e+01]\n",
      " [2.012e+03 3.000e+00 1.500e+01]\n",
      " [2.012e+03 4.000e+00 1.500e+01]\n",
      " [2.012e+03 5.000e+00 1.500e+01]\n",
      " [2.012e+03 6.000e+00 1.500e+01]\n",
      " [2.012e+03 7.000e+00 1.500e+01]\n",
      " [2.012e+03 8.000e+00 1.500e+01]\n",
      " [2.012e+03 9.000e+00 1.500e+01]\n",
      " [2.012e+03 1.000e+01 1.500e+01]\n",
      " [2.012e+03 1.100e+01 1.500e+01]\n",
      " [2.012e+03 1.200e+01 1.500e+01]\n",
      " [2.013e+03 1.000e+00 1.500e+01]\n",
      " [2.013e+03 2.000e+00 1.500e+01]\n",
      " [2.013e+03 3.000e+00 1.500e+01]\n",
      " [2.013e+03 4.000e+00 1.500e+01]\n",
      " [2.013e+03 5.000e+00 1.500e+01]\n",
      " [2.013e+03 6.000e+00 1.500e+01]\n",
      " [2.013e+03 7.000e+00 1.500e+01]\n",
      " [2.013e+03 8.000e+00 1.500e+01]\n",
      " [2.013e+03 9.000e+00 1.500e+01]\n",
      " [2.013e+03 1.000e+01 1.500e+01]\n",
      " [2.013e+03 1.100e+01 1.500e+01]\n",
      " [2.013e+03 1.200e+01 1.500e+01]\n",
      " [2.014e+03 1.000e+00 1.500e+01]\n",
      " [2.014e+03 2.000e+00 1.500e+01]\n",
      " [2.014e+03 3.000e+00 1.500e+01]\n",
      " [2.014e+03 4.000e+00 1.500e+01]\n",
      " [2.014e+03 5.000e+00 1.500e+01]\n",
      " [2.014e+03 6.000e+00 1.500e+01]\n",
      " [2.014e+03 7.000e+00 1.500e+01]\n",
      " [2.014e+03 8.000e+00 1.500e+01]\n",
      " [2.014e+03 9.000e+00 1.500e+01]\n",
      " [2.014e+03 1.000e+01 1.500e+01]\n",
      " [2.014e+03 1.100e+01 1.500e+01]\n",
      " [2.014e+03 1.200e+01 1.500e+01]\n",
      " [2.015e+03 1.000e+00 1.500e+01]\n",
      " [2.015e+03 2.000e+00 1.500e+01]\n",
      " [2.015e+03 3.000e+00 1.500e+01]\n",
      " [2.015e+03 4.000e+00 1.500e+01]\n",
      " [2.015e+03 5.000e+00 1.500e+01]\n",
      " [2.015e+03 6.000e+00 1.500e+01]\n",
      " [2.015e+03 7.000e+00 1.500e+01]\n",
      " [2.015e+03 8.000e+00 1.500e+01]\n",
      " [2.015e+03 9.000e+00 1.500e+01]\n",
      " [2.015e+03 1.000e+01 1.500e+01]\n",
      " [2.015e+03 1.100e+01 1.500e+01]\n",
      " [2.015e+03 1.200e+01 1.500e+01]\n",
      " [2.016e+03 1.000e+00 1.500e+01]\n",
      " [2.016e+03 2.000e+00 1.500e+01]\n",
      " [2.016e+03 3.000e+00 1.500e+01]\n",
      " [2.016e+03 4.000e+00 1.500e+01]\n",
      " [2.016e+03 5.000e+00 1.500e+01]\n",
      " [2.016e+03 6.000e+00 1.500e+01]\n",
      " [2.016e+03 7.000e+00 1.500e+01]\n",
      " [2.016e+03 8.000e+00 1.500e+01]\n",
      " [2.016e+03 9.000e+00 1.500e+01]\n",
      " [2.016e+03 1.000e+01 1.500e+01]\n",
      " [2.016e+03 1.100e+01 1.500e+01]\n",
      " [2.016e+03 1.200e+01 1.500e+01]] [[2.002e+03 7.000e+00 1.500e+01]\n",
      " [2.002e+03 8.000e+00 1.500e+01]\n",
      " [2.002e+03 9.000e+00 1.500e+01]\n",
      " [2.002e+03 1.000e+01 1.500e+01]\n",
      " [2.002e+03 1.100e+01 1.500e+01]\n",
      " [2.002e+03 1.200e+01 1.500e+01]\n",
      " [2.003e+03 1.000e+00 1.500e+01]\n",
      " [2.003e+03 2.000e+00 1.500e+01]\n",
      " [2.003e+03 3.000e+00 1.500e+01]\n",
      " [2.003e+03 4.000e+00 1.500e+01]\n",
      " [2.003e+03 5.000e+00 1.500e+01]\n",
      " [2.003e+03 6.000e+00 1.500e+01]\n",
      " [2.003e+03 7.000e+00 1.500e+01]\n",
      " [2.003e+03 8.000e+00 1.500e+01]\n",
      " [2.003e+03 9.000e+00 1.500e+01]\n",
      " [2.003e+03 1.000e+01 1.500e+01]\n",
      " [2.003e+03 1.100e+01 1.500e+01]\n",
      " [2.003e+03 1.200e+01 1.500e+01]\n",
      " [2.004e+03 1.000e+00 1.500e+01]\n",
      " [2.004e+03 2.000e+00 1.500e+01]\n",
      " [2.004e+03 3.000e+00 1.500e+01]\n",
      " [2.004e+03 4.000e+00 1.500e+01]\n",
      " [2.004e+03 5.000e+00 1.500e+01]\n",
      " [2.004e+03 6.000e+00 1.500e+01]\n",
      " [2.004e+03 7.000e+00 1.500e+01]\n",
      " [2.004e+03 8.000e+00 1.500e+01]\n",
      " [2.004e+03 9.000e+00 1.500e+01]\n",
      " [2.004e+03 1.000e+01 1.500e+01]\n",
      " [2.004e+03 1.100e+01 1.500e+01]\n",
      " [2.004e+03 1.200e+01 1.500e+01]\n",
      " [2.005e+03 1.000e+00 1.500e+01]\n",
      " [2.005e+03 2.000e+00 1.500e+01]\n",
      " [2.005e+03 3.000e+00 1.500e+01]\n",
      " [2.005e+03 4.000e+00 1.500e+01]\n",
      " [2.005e+03 5.000e+00 1.500e+01]\n",
      " [2.005e+03 6.000e+00 1.500e+01]\n",
      " [2.005e+03 7.000e+00 1.500e+01]\n",
      " [2.005e+03 8.000e+00 1.500e+01]\n",
      " [2.005e+03 9.000e+00 1.500e+01]\n",
      " [2.005e+03 1.000e+01 1.500e+01]\n",
      " [2.005e+03 1.100e+01 1.500e+01]\n",
      " [2.005e+03 1.200e+01 1.500e+01]\n",
      " [2.006e+03 1.000e+00 1.500e+01]\n",
      " [2.006e+03 2.000e+00 1.500e+01]\n",
      " [2.006e+03 3.000e+00 1.500e+01]\n",
      " [2.006e+03 4.000e+00 1.500e+01]\n",
      " [2.006e+03 5.000e+00 1.500e+01]\n",
      " [2.006e+03 6.000e+00 1.500e+01]\n",
      " [2.006e+03 7.000e+00 1.500e+01]\n",
      " [2.006e+03 8.000e+00 1.500e+01]\n",
      " [2.006e+03 9.000e+00 1.500e+01]\n",
      " [2.006e+03 1.000e+01 1.500e+01]\n",
      " [2.006e+03 1.100e+01 1.500e+01]\n",
      " [2.006e+03 1.200e+01 1.500e+01]\n",
      " [2.007e+03 1.000e+00 1.500e+01]\n",
      " [2.007e+03 2.000e+00 1.500e+01]\n",
      " [2.007e+03 3.000e+00 1.500e+01]\n",
      " [2.007e+03 4.000e+00 1.500e+01]\n",
      " [2.007e+03 5.000e+00 1.500e+01]\n",
      " [2.007e+03 6.000e+00 1.500e+01]\n",
      " [2.007e+03 7.000e+00 1.500e+01]\n",
      " [2.007e+03 8.000e+00 1.500e+01]\n",
      " [2.007e+03 9.000e+00 1.500e+01]\n",
      " [2.007e+03 1.000e+01 1.500e+01]\n",
      " [2.007e+03 1.100e+01 1.500e+01]\n",
      " [2.007e+03 1.200e+01 1.500e+01]\n",
      " [2.008e+03 1.000e+00 1.500e+01]\n",
      " [2.008e+03 2.000e+00 1.500e+01]\n",
      " [2.008e+03 3.000e+00 1.500e+01]\n",
      " [2.008e+03 4.000e+00 1.500e+01]\n",
      " [2.008e+03 5.000e+00 1.500e+01]\n",
      " [2.008e+03 6.000e+00 1.500e+01]\n",
      " [2.008e+03 7.000e+00 1.500e+01]\n",
      " [2.008e+03 8.000e+00 1.500e+01]\n",
      " [2.008e+03 9.000e+00 1.500e+01]\n",
      " [2.008e+03 1.000e+01 1.500e+01]\n",
      " [2.008e+03 1.100e+01 1.500e+01]\n",
      " [2.008e+03 1.200e+01 1.500e+01]\n",
      " [2.009e+03 1.000e+00 1.500e+01]\n",
      " [2.009e+03 2.000e+00 1.500e+01]\n",
      " [2.009e+03 3.000e+00 1.500e+01]\n",
      " [2.009e+03 4.000e+00 1.500e+01]\n",
      " [2.009e+03 5.000e+00 1.500e+01]\n",
      " [2.009e+03 6.000e+00 1.500e+01]\n",
      " [2.009e+03 7.000e+00 1.500e+01]\n",
      " [2.009e+03 8.000e+00 1.500e+01]\n",
      " [2.009e+03 9.000e+00 1.500e+01]\n",
      " [2.009e+03 1.000e+01 1.500e+01]\n",
      " [2.009e+03 1.100e+01 1.500e+01]\n",
      " [2.009e+03 1.200e+01 1.500e+01]\n",
      " [2.010e+03 1.000e+00 1.500e+01]\n",
      " [2.010e+03 2.000e+00 1.500e+01]\n",
      " [2.010e+03 3.000e+00 1.500e+01]\n",
      " [2.010e+03 4.000e+00 1.500e+01]\n",
      " [2.010e+03 5.000e+00 1.500e+01]\n",
      " [2.010e+03 6.000e+00 1.500e+01]\n",
      " [2.010e+03 7.000e+00 1.500e+01]\n",
      " [2.010e+03 8.000e+00 1.500e+01]\n",
      " [2.010e+03 9.000e+00 1.500e+01]\n",
      " [2.010e+03 1.000e+01 1.500e+01]\n",
      " [2.010e+03 1.100e+01 1.500e+01]\n",
      " [2.010e+03 1.200e+01 1.500e+01]\n",
      " [2.011e+03 1.000e+00 1.500e+01]\n",
      " [2.011e+03 2.000e+00 1.500e+01]\n",
      " [2.011e+03 3.000e+00 1.500e+01]\n",
      " [2.011e+03 4.000e+00 1.500e+01]\n",
      " [2.011e+03 5.000e+00 1.500e+01]\n",
      " [2.011e+03 6.000e+00 1.500e+01]\n",
      " [2.011e+03 7.000e+00 1.500e+01]\n",
      " [2.011e+03 8.000e+00 1.500e+01]\n",
      " [2.011e+03 9.000e+00 1.500e+01]\n",
      " [2.011e+03 1.000e+01 1.500e+01]\n",
      " [2.011e+03 1.100e+01 1.500e+01]\n",
      " [2.011e+03 1.200e+01 1.500e+01]\n",
      " [2.012e+03 1.000e+00 1.500e+01]\n",
      " [2.012e+03 2.000e+00 1.500e+01]\n",
      " [2.012e+03 3.000e+00 1.500e+01]\n",
      " [2.012e+03 4.000e+00 1.500e+01]\n",
      " [2.012e+03 5.000e+00 1.500e+01]\n",
      " [2.012e+03 6.000e+00 1.500e+01]\n",
      " [2.012e+03 7.000e+00 1.500e+01]\n",
      " [2.012e+03 8.000e+00 1.500e+01]\n",
      " [2.012e+03 9.000e+00 1.500e+01]\n",
      " [2.012e+03 1.000e+01 1.500e+01]\n",
      " [2.012e+03 1.100e+01 1.500e+01]\n",
      " [2.012e+03 1.200e+01 1.500e+01]\n",
      " [2.013e+03 1.000e+00 1.500e+01]\n",
      " [2.013e+03 2.000e+00 1.500e+01]\n",
      " [2.013e+03 3.000e+00 1.500e+01]\n",
      " [2.013e+03 4.000e+00 1.500e+01]\n",
      " [2.013e+03 5.000e+00 1.500e+01]\n",
      " [2.013e+03 6.000e+00 1.500e+01]\n",
      " [2.013e+03 7.000e+00 1.500e+01]\n",
      " [2.013e+03 8.000e+00 1.500e+01]\n",
      " [2.013e+03 9.000e+00 1.500e+01]\n",
      " [2.013e+03 1.000e+01 1.500e+01]\n",
      " [2.013e+03 1.100e+01 1.500e+01]\n",
      " [2.013e+03 1.200e+01 1.500e+01]\n",
      " [2.014e+03 1.000e+00 1.500e+01]\n",
      " [2.014e+03 2.000e+00 1.500e+01]\n",
      " [2.014e+03 3.000e+00 1.500e+01]\n",
      " [2.014e+03 4.000e+00 1.500e+01]\n",
      " [2.014e+03 5.000e+00 1.500e+01]\n",
      " [2.014e+03 6.000e+00 1.500e+01]\n",
      " [2.014e+03 7.000e+00 1.500e+01]\n",
      " [2.014e+03 8.000e+00 1.500e+01]\n",
      " [2.014e+03 9.000e+00 1.500e+01]\n",
      " [2.014e+03 1.000e+01 1.500e+01]\n",
      " [2.014e+03 1.100e+01 1.500e+01]\n",
      " [2.014e+03 1.200e+01 1.500e+01]\n",
      " [2.015e+03 1.000e+00 1.500e+01]\n",
      " [2.015e+03 2.000e+00 1.500e+01]\n",
      " [2.015e+03 3.000e+00 1.500e+01]\n",
      " [2.015e+03 4.000e+00 1.500e+01]\n",
      " [2.015e+03 5.000e+00 1.500e+01]\n",
      " [2.015e+03 6.000e+00 1.500e+01]\n",
      " [2.015e+03 7.000e+00 1.500e+01]\n",
      " [2.015e+03 8.000e+00 1.500e+01]\n",
      " [2.015e+03 9.000e+00 1.500e+01]\n",
      " [2.015e+03 1.000e+01 1.500e+01]\n",
      " [2.015e+03 1.100e+01 1.500e+01]\n",
      " [2.015e+03 1.200e+01 1.500e+01]\n",
      " [2.016e+03 1.000e+00 1.500e+01]\n",
      " [2.016e+03 2.000e+00 1.500e+01]\n",
      " [2.016e+03 3.000e+00 1.500e+01]\n",
      " [2.016e+03 4.000e+00 1.500e+01]\n",
      " [2.016e+03 5.000e+00 1.500e+01]\n",
      " [2.016e+03 6.000e+00 1.500e+01]\n",
      " [2.016e+03 7.000e+00 1.500e+01]\n",
      " [2.016e+03 8.000e+00 1.500e+01]\n",
      " [2.016e+03 9.000e+00 1.500e+01]\n",
      " [2.016e+03 1.000e+01 1.500e+01]\n",
      " [2.016e+03 1.100e+01 1.500e+01]\n",
      " [2.016e+03 1.200e+01 1.500e+01]] [[2.002e+03 7.000e+00 1.500e+01]\n",
      " [2.002e+03 8.000e+00 1.500e+01]\n",
      " [2.002e+03 9.000e+00 1.500e+01]\n",
      " [2.002e+03 1.000e+01 1.500e+01]\n",
      " [2.002e+03 1.100e+01 1.500e+01]\n",
      " [2.002e+03 1.200e+01 1.500e+01]\n",
      " [2.003e+03 1.000e+00 1.500e+01]\n",
      " [2.003e+03 2.000e+00 1.500e+01]\n",
      " [2.003e+03 3.000e+00 1.500e+01]\n",
      " [2.003e+03 4.000e+00 1.500e+01]\n",
      " [2.003e+03 5.000e+00 1.500e+01]\n",
      " [2.003e+03 6.000e+00 1.500e+01]\n",
      " [2.003e+03 7.000e+00 1.500e+01]\n",
      " [2.003e+03 8.000e+00 1.500e+01]\n",
      " [2.003e+03 9.000e+00 1.500e+01]\n",
      " [2.003e+03 1.000e+01 1.500e+01]\n",
      " [2.003e+03 1.100e+01 1.500e+01]\n",
      " [2.003e+03 1.200e+01 1.500e+01]\n",
      " [2.004e+03 1.000e+00 1.500e+01]\n",
      " [2.004e+03 2.000e+00 1.500e+01]\n",
      " [2.004e+03 3.000e+00 1.500e+01]\n",
      " [2.004e+03 4.000e+00 1.500e+01]\n",
      " [2.004e+03 5.000e+00 1.500e+01]\n",
      " [2.004e+03 6.000e+00 1.500e+01]\n",
      " [2.004e+03 7.000e+00 1.500e+01]\n",
      " [2.004e+03 8.000e+00 1.500e+01]\n",
      " [2.004e+03 9.000e+00 1.500e+01]\n",
      " [2.004e+03 1.000e+01 1.500e+01]\n",
      " [2.004e+03 1.100e+01 1.500e+01]\n",
      " [2.004e+03 1.200e+01 1.500e+01]\n",
      " [2.005e+03 1.000e+00 1.500e+01]\n",
      " [2.005e+03 2.000e+00 1.500e+01]\n",
      " [2.005e+03 3.000e+00 1.500e+01]\n",
      " [2.005e+03 4.000e+00 1.500e+01]\n",
      " [2.005e+03 5.000e+00 1.500e+01]\n",
      " [2.005e+03 6.000e+00 1.500e+01]\n",
      " [2.005e+03 7.000e+00 1.500e+01]\n",
      " [2.005e+03 8.000e+00 1.500e+01]\n",
      " [2.005e+03 9.000e+00 1.500e+01]\n",
      " [2.005e+03 1.000e+01 1.500e+01]\n",
      " [2.005e+03 1.100e+01 1.500e+01]\n",
      " [2.005e+03 1.200e+01 1.500e+01]\n",
      " [2.006e+03 1.000e+00 1.500e+01]\n",
      " [2.006e+03 2.000e+00 1.500e+01]\n",
      " [2.006e+03 3.000e+00 1.500e+01]\n",
      " [2.006e+03 4.000e+00 1.500e+01]\n",
      " [2.006e+03 5.000e+00 1.500e+01]\n",
      " [2.006e+03 6.000e+00 1.500e+01]\n",
      " [2.006e+03 7.000e+00 1.500e+01]\n",
      " [2.006e+03 8.000e+00 1.500e+01]\n",
      " [2.006e+03 9.000e+00 1.500e+01]\n",
      " [2.006e+03 1.000e+01 1.500e+01]\n",
      " [2.006e+03 1.100e+01 1.500e+01]\n",
      " [2.006e+03 1.200e+01 1.500e+01]\n",
      " [2.007e+03 1.000e+00 1.500e+01]\n",
      " [2.007e+03 2.000e+00 1.500e+01]\n",
      " [2.007e+03 3.000e+00 1.500e+01]\n",
      " [2.007e+03 4.000e+00 1.500e+01]\n",
      " [2.007e+03 5.000e+00 1.500e+01]\n",
      " [2.007e+03 6.000e+00 1.500e+01]\n",
      " [2.007e+03 7.000e+00 1.500e+01]\n",
      " [2.007e+03 8.000e+00 1.500e+01]\n",
      " [2.007e+03 9.000e+00 1.500e+01]\n",
      " [2.007e+03 1.000e+01 1.500e+01]\n",
      " [2.007e+03 1.100e+01 1.500e+01]\n",
      " [2.007e+03 1.200e+01 1.500e+01]\n",
      " [2.008e+03 1.000e+00 1.500e+01]\n",
      " [2.008e+03 2.000e+00 1.500e+01]\n",
      " [2.008e+03 3.000e+00 1.500e+01]\n",
      " [2.008e+03 4.000e+00 1.500e+01]\n",
      " [2.008e+03 5.000e+00 1.500e+01]\n",
      " [2.008e+03 6.000e+00 1.500e+01]\n",
      " [2.008e+03 7.000e+00 1.500e+01]\n",
      " [2.008e+03 8.000e+00 1.500e+01]\n",
      " [2.008e+03 9.000e+00 1.500e+01]\n",
      " [2.008e+03 1.000e+01 1.500e+01]\n",
      " [2.008e+03 1.100e+01 1.500e+01]\n",
      " [2.008e+03 1.200e+01 1.500e+01]\n",
      " [2.009e+03 1.000e+00 1.500e+01]\n",
      " [2.009e+03 2.000e+00 1.500e+01]\n",
      " [2.009e+03 3.000e+00 1.500e+01]\n",
      " [2.009e+03 4.000e+00 1.500e+01]\n",
      " [2.009e+03 5.000e+00 1.500e+01]\n",
      " [2.009e+03 6.000e+00 1.500e+01]\n",
      " [2.009e+03 7.000e+00 1.500e+01]\n",
      " [2.009e+03 8.000e+00 1.500e+01]\n",
      " [2.009e+03 9.000e+00 1.500e+01]\n",
      " [2.009e+03 1.000e+01 1.500e+01]\n",
      " [2.009e+03 1.100e+01 1.500e+01]\n",
      " [2.009e+03 1.200e+01 1.500e+01]\n",
      " [2.010e+03 1.000e+00 1.500e+01]\n",
      " [2.010e+03 2.000e+00 1.500e+01]\n",
      " [2.010e+03 3.000e+00 1.500e+01]\n",
      " [2.010e+03 4.000e+00 1.500e+01]\n",
      " [2.010e+03 5.000e+00 1.500e+01]\n",
      " [2.010e+03 6.000e+00 1.500e+01]\n",
      " [2.010e+03 7.000e+00 1.500e+01]\n",
      " [2.010e+03 8.000e+00 1.500e+01]\n",
      " [2.010e+03 9.000e+00 1.500e+01]\n",
      " [2.010e+03 1.000e+01 1.500e+01]\n",
      " [2.010e+03 1.100e+01 1.500e+01]\n",
      " [2.010e+03 1.200e+01 1.500e+01]\n",
      " [2.011e+03 1.000e+00 1.500e+01]\n",
      " [2.011e+03 2.000e+00 1.500e+01]\n",
      " [2.011e+03 3.000e+00 1.500e+01]\n",
      " [2.011e+03 4.000e+00 1.500e+01]\n",
      " [2.011e+03 5.000e+00 1.500e+01]\n",
      " [2.011e+03 6.000e+00 1.500e+01]\n",
      " [2.011e+03 7.000e+00 1.500e+01]\n",
      " [2.011e+03 8.000e+00 1.500e+01]\n",
      " [2.011e+03 9.000e+00 1.500e+01]\n",
      " [2.011e+03 1.000e+01 1.500e+01]\n",
      " [2.011e+03 1.100e+01 1.500e+01]\n",
      " [2.011e+03 1.200e+01 1.500e+01]\n",
      " [2.012e+03 1.000e+00 1.500e+01]\n",
      " [2.012e+03 2.000e+00 1.500e+01]\n",
      " [2.012e+03 3.000e+00 1.500e+01]\n",
      " [2.012e+03 4.000e+00 1.500e+01]\n",
      " [2.012e+03 5.000e+00 1.500e+01]\n",
      " [2.012e+03 6.000e+00 1.500e+01]\n",
      " [2.012e+03 7.000e+00 1.500e+01]\n",
      " [2.012e+03 8.000e+00 1.500e+01]\n",
      " [2.012e+03 9.000e+00 1.500e+01]\n",
      " [2.012e+03 1.000e+01 1.500e+01]\n",
      " [2.012e+03 1.100e+01 1.500e+01]\n",
      " [2.012e+03 1.200e+01 1.500e+01]\n",
      " [2.013e+03 1.000e+00 1.500e+01]\n",
      " [2.013e+03 2.000e+00 1.500e+01]\n",
      " [2.013e+03 3.000e+00 1.500e+01]\n",
      " [2.013e+03 4.000e+00 1.500e+01]\n",
      " [2.013e+03 5.000e+00 1.500e+01]\n",
      " [2.013e+03 6.000e+00 1.500e+01]\n",
      " [2.013e+03 7.000e+00 1.500e+01]\n",
      " [2.013e+03 8.000e+00 1.500e+01]\n",
      " [2.013e+03 9.000e+00 1.500e+01]\n",
      " [2.013e+03 1.000e+01 1.500e+01]\n",
      " [2.013e+03 1.100e+01 1.500e+01]\n",
      " [2.013e+03 1.200e+01 1.500e+01]\n",
      " [2.014e+03 1.000e+00 1.500e+01]\n",
      " [2.014e+03 2.000e+00 1.500e+01]\n",
      " [2.014e+03 3.000e+00 1.500e+01]\n",
      " [2.014e+03 4.000e+00 1.500e+01]\n",
      " [2.014e+03 5.000e+00 1.500e+01]\n",
      " [2.014e+03 6.000e+00 1.500e+01]\n",
      " [2.014e+03 7.000e+00 1.500e+01]\n",
      " [2.014e+03 8.000e+00 1.500e+01]\n",
      " [2.014e+03 9.000e+00 1.500e+01]\n",
      " [2.014e+03 1.000e+01 1.500e+01]\n",
      " [2.014e+03 1.100e+01 1.500e+01]\n",
      " [2.014e+03 1.200e+01 1.500e+01]\n",
      " [2.015e+03 1.000e+00 1.500e+01]\n",
      " [2.015e+03 2.000e+00 1.500e+01]\n",
      " [2.015e+03 3.000e+00 1.500e+01]\n",
      " [2.015e+03 4.000e+00 1.500e+01]\n",
      " [2.015e+03 5.000e+00 1.500e+01]\n",
      " [2.015e+03 6.000e+00 1.500e+01]\n",
      " [2.015e+03 7.000e+00 1.500e+01]\n",
      " [2.015e+03 8.000e+00 1.500e+01]\n",
      " [2.015e+03 9.000e+00 1.500e+01]\n",
      " [2.015e+03 1.000e+01 1.500e+01]\n",
      " [2.015e+03 1.100e+01 1.500e+01]\n",
      " [2.015e+03 1.200e+01 1.500e+01]\n",
      " [2.016e+03 1.000e+00 1.500e+01]\n",
      " [2.016e+03 2.000e+00 1.500e+01]\n",
      " [2.016e+03 3.000e+00 1.500e+01]\n",
      " [2.016e+03 4.000e+00 1.500e+01]\n",
      " [2.016e+03 5.000e+00 1.500e+01]\n",
      " [2.016e+03 6.000e+00 1.500e+01]\n",
      " [2.016e+03 7.000e+00 1.500e+01]\n",
      " [2.016e+03 8.000e+00 1.500e+01]\n",
      " [2.016e+03 9.000e+00 1.500e+01]\n",
      " [2.016e+03 1.000e+01 1.500e+01]\n",
      " [2.016e+03 1.100e+01 1.500e+01]\n",
      " [2.016e+03 1.200e+01 1.500e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(rsut[0, :, 179])\n",
    "print(rsdt[0, :, 179])\n",
    "print(rsutcs[0, :, 179])\n",
    "\n",
    "albedo = rsut / rsdt\n",
    "albedo_cs = rsutcs / rsdt\n",
    "print(albedo[1,:,179])\n",
    "print(albedo_cs[1, :, 179])\n",
    "\n",
    "print(time_ceres, time_ceres2, time_ceres3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb524ae-7953-42b0-837f-e4ce46e88c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-84.5 -83.5 -82.5 -81.5 -80.5 -79.5 -78.5 -77.5 -76.5 -75.5 -74.5 -73.5\n",
      " -72.5 -71.5 -70.5 -69.5 -68.5 -67.5 -66.5 -65.5 -64.5 -63.5 -62.5 -61.5\n",
      " -60.5 -59.5 -58.5 -57.5 -56.5 -55.5 -54.5 -53.5 -52.5 -51.5 -50.5 -49.5\n",
      " -48.5 -47.5 -46.5 -45.5 -44.5 -43.5 -42.5 -41.5 -40.5]\n"
     ]
    }
   ],
   "source": [
    "print(lat_ceres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07dbc8c-8b08-4e41-9340-3303c46704ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02b0a5-49f9-4b4e-934a-2acfdb09c61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c203da4-5177-4276-9f51-674ea7452487",
   "metadata": {},
   "source": [
    "## pdf of '$\\alpha _{cs}$' vs 'MAC-LWP shows as Land/Sea-Ice' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e23646-6674-4057-835b-798f78e82277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "# --Liquid water path, Unit in kg m^-2\n",
    "LWP = inputVar_obs['lwp'] / 1000.\n",
    "# 1-Sigma Liquid water path statistic error, Unit in Kg m^-2\n",
    "LWP_error = inputVar_obs['lwp_error'] / 1000.\n",
    "# the MaskedArray of 'MAC-LWP' dataset\n",
    "Maskarray_mac = inputVar_obs['maskarray_mac']\n",
    "# ---\n",
    "\n",
    "# GMT: Global mean surface air Temperature (2-meter), Unit in K\n",
    "gmt = inputVar_obs['tas'] * 1.\n",
    "# SST: Sea Surface Temperature or skin- Temperature, Unit in K\n",
    "SST = inputVar_obs['sfc_T'] * 1.\n",
    "# Precip: Precipitation, Unit in mm day^-1 (convert from kg m^-2 s^-1)\n",
    "Precip = inputVar_obs['P'] * (24. * 60 * 60)\n",
    "# Eva: Evaporation, Unit in mm day^-1 (here use the latent heat flux from the sfc, unit convert from W m^-2 --> kg m^-2 s^-1 --> mm day^-1)\n",
    "lh_vaporization = (2.501 - (2.361 * 10**-3) * (SST - 273.15)) * 1e6  # the latent heat of vaporization at the surface Temperature\n",
    "Eva = inputVar_obs['E'] / lh_vaporization * (24. * 60 * 60)\n",
    "\n",
    "# MC: Moisture Convergence, represent the water vapor abundance, Unit in mm day^-1\n",
    "MC = Precip - Eva\n",
    "print(MC)\n",
    "\n",
    "# LTS: Lower Tropospheric Stability, Unit in K (the same as Potential Temperature):\n",
    "k = 0.286\n",
    "\n",
    "theta_700 = inputVar_obs['T_700'] * (100000. / 70000.)**k\n",
    "theta_skin = inputVar_obs['sfc_T'] * (100000. / inputVar_obs['sfc_P'])**k\n",
    "LTS_m = theta_700 - theta_skin  # LTS with np.nan\n",
    "\n",
    "#.. mask the place with np.nan value\n",
    "LTS_e = np.ma.masked_where(theta_700==np.nan, LTS_m)\n",
    "# print(LTS_e)\n",
    "\n",
    "Subsidence = inputVar_obs['sub']\n",
    "\n",
    "# SW radiative flux:\n",
    "Rsdt = inputVar_obs['rsdt']\n",
    "Rsut = inputVar_obs['rsut']\n",
    "Rsutcs = inputVar_obs['rsutcs']\n",
    "\n",
    "albedo = Rsut / Rsdt\n",
    "albedo_cs = Rsutcs / Rsdt\n",
    "Alpha_cre = albedo - albedo_cs\n",
    "\n",
    "# abnormal values:\n",
    "# albedo_cs[(albedo_cs <= 0.04) & (albedo_cs >= 1.00)] == np.nan\n",
    "# Alpha_cre[(albedo_cs <= 0.04) & (albedo_cs >= 1.00)] == np.nan\n",
    "# define Dictionary to store: CCFs(4), gmt, other variables :\n",
    "\n",
    "dict0_var = {'gmt': gmt, 'SST': SST, 'p_e': MC, 'LTS': LTS_e, 'SUB': Subsidence, 'LWP': LWP, 'rsdt': Rsdt, 'rsut': Rsut, 'rsutcs': Rsutcs, 'albedo' : albedo, 'albedo_cs': albedo_cs, 'alpha_cre': Alpha_cre, 'LWP_statistic_error': LWP_error, 'Maskarray_mac': Maskarray_mac}\n",
    "\n",
    "# Crop the regions\n",
    "# crop the variables to the Southern Ocean latitude range: (40 ~ 85^o S)\n",
    "\n",
    "variable_nas = ['SST', 'p_e', 'LTS', 'SUB', 'LWP', 'LWP_statistic_error', 'rsdt', 'rsut', 'rsutcs', 'albedo', 'albedo_cs', 'alpha_cre']\n",
    "\n",
    "dict1_SO, lat_merra2_so, lon_merra2_so = region_cropping(dict0_var, ['SST', 'p_e', 'LTS', 'SUB'], inputVar_obs['lat_merra2'], inputVar_obs['lon_merra2'], lat_range = [-85., -40.], lon_range = [-180., 180.])\n",
    "    \n",
    "dict1_SO, lat_mac_so, lon_mac_so = region_cropping(dict1_SO, ['LWP', 'LWP_statistic_error', 'Maskarray_mac'], inputVar_obs['lat_mac'], inputVar_obs['lon_mac'], lat_range =[-85., -40.], lon_range = [-180., 180.])\n",
    "\n",
    "\n",
    "mask_int = np.zeros((dict1_SO['LWP'].shape))\n",
    "print(mask_int.shape)\n",
    "\n",
    "mask_int[dict1_SO['Maskarray_mac'] == True] = 1.\n",
    "\n",
    "Count_True_binned, binned_edges, binnumber = stats.binned_statistic(dict1_SO['albedo_cs'].flatten(), mask_int.flatten(), statistic = 'sum', bins = np.arange(0.0, 1, 0.02))\n",
    "Count_Sum_binned = stats.binned_statistic(dict1_SO['albedo_cs'].flatten(), mask_int.flatten(), statistic = 'count', bins = np.arange(0.0, 1, 0.02))[0]\n",
    "probability = Count_True_binned / Count_Sum_binned\n",
    "print(probablity)\n",
    "# F_landseaIce = Count_binned / np.asarray(np.nonzero(mask_int == 1.)).shape[1]\n",
    "print(np.sum(F_landdseaIce))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49473415-12b1-4798-a1fb-6dc1af5a4e8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probability' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-f740627879e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'--'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'probability' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path6 = '/glade/work/chuyan/Research/Cloud_CCFs_RMs/Course_objective_ana/plot_file/plots_Sep8_Observation_data/'\n",
    "plt.figure( figsize = (10, 7.) )\n",
    "parameters = {'axes.labelsize': 18, 'xtick.labelsize': 16, 'ytick.labelsize': 16, \n",
    "         'axes.titlesize': 21, 'legend.fontsize': 20}\n",
    "plt.rcParams.update(parameters)\n",
    "\n",
    "\n",
    "x = np.arange(0.00, 0.98, 0.02)\n",
    "y = probability\n",
    "\n",
    "plt.plot(x, y, linestyle = '--', c = 'b', linewidth = 2.)\n",
    "plt.xlabel(r\"$ Clear-Sky\\ \\alpha \\ $\")\n",
    "plt.ylabel(\"Probability \")\n",
    "\n",
    "plt.title(\" where the 'MAC-lwp' dataset has no data\")\n",
    "plt.savefig(path6+'Probability_with_nodata_in_MAC-LWP.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be5724-78f4-4f98-ad85-165ed68772e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# calc_LRMobs_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2af0329-4837-4f13-9686-914f92881358",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_flag = 'test1'\n",
    "\n",
    "# get the variable:\n",
    "inputVar_obs = get_OBSLRM(test = test_flag)\n",
    "# ------------------------ \n",
    "# radiation code\n",
    "\n",
    "# ------------------------\n",
    "\n",
    "# Data processing\n",
    "# --Liquid water path, Unit in kg m^-2\n",
    "LWP = inputVar_obs['lwp'] / 1000.\n",
    "# 1-Sigma Liquid water path statistic error, Unit in Kg m^-2\n",
    "LWP_error = inputVar_obs['lwp_error'] / 1000.\n",
    "# the MaskedArray of 'MAC-LWP' dataset\n",
    "Maskarray_mac = inputVar_obs['maskarray_mac']\n",
    "# ---\n",
    "\n",
    "# GMT: Global mean surface air Temperature (2-meter), Unit in K\n",
    "gmt = inputVar_obs['tas'] * 1.\n",
    "# SST: Sea Surface Temperature or skin- Temperature, Unit in K\n",
    "SST = inputVar_obs['sfc_T'] * 1.\n",
    "# Precip: Precipitation, Unit in mm day^-1 (convert from kg m^-2 s^-1)\n",
    "Precip = inputVar_obs['P'] * (24. * 60 * 60)\n",
    "# Eva: Evaporation, Unit in mm day^-1 (here use the latent heat flux from the sfc, unit convert from W m^-2 --> kg m^-2 s^-1 --> mm day^-1)\n",
    "lh_vaporization = (2.501 - (2.361 * 10**-3) * (SST - 273.15)) * 1e6  # the latent heat of vaporization at the surface Temperature\n",
    "Eva = inputVar_obs['E'] / lh_vaporization * (24. * 60 * 60)\n",
    "\n",
    "# MC: Moisture Convergence, represent the water vapor abundance, Unit in mm day^-1\n",
    "MC = Precip - Eva\n",
    "print(MC)\n",
    "\n",
    "# LTS: Lower Tropospheric Stability, Unit in K (the same as Potential Temperature):\n",
    "k = 0.286\n",
    "\n",
    "theta_700 = inputVar_obs['T_700'] * (100000. / 70000.)**k\n",
    "theta_skin = inputVar_obs['sfc_T'] * (100000. / inputVar_obs['sfc_P'])**k\n",
    "LTS_m = theta_700 - theta_skin  # LTS with np.nan\n",
    "\n",
    "#.. mask the place with np.nan value\n",
    "LTS_e = np.ma.masked_where(theta_700==np.nan, LTS_m)\n",
    "# print(LTS_e)\n",
    "\n",
    "Subsidence = inputVar_obs['sub']\n",
    "\n",
    "# SW radiative flux:\n",
    "Rsdt = inputVar_obs['rsdt']\n",
    "Rsut = inputVar_obs['rsut']\n",
    "Rsutcs = inputVar_obs['rsutcs']\n",
    "\n",
    "albedo = Rsut / Rsdt\n",
    "albedo_cs = Rsutcs / Rsdt\n",
    "Alpha_cre = albedo - albedo_cs\n",
    "# abnormal values:\n",
    "albedo_cs[(albedo_cs <= 0.08) & (albedo_cs >= 1.00)] == np.nan\n",
    "Alpha_cre[(albedo_cs <= 0.08) & (albedo_cs >= 1.00)] == np.nan\n",
    "\n",
    "# define Dictionary to store: CCFs(4), gmt, other variables :\n",
    "dict0_var = {'gmt': gmt, 'SST': SST, 'p_e': MC, 'LTS': LTS_e, 'SUB': Subsidence, 'LWP': LWP, 'rsdt': Rsdt, 'rsut': Rsut, 'rsutcs': Rsutcs, 'albedo' : albedo, 'albedo_cs': albedo_cs, 'alpha_cre': Alpha_cre, 'LWP_statistic_error': LWP_error, 'Maskarray_mac': Maskarray_mac}\n",
    "\n",
    "# Crop the regions\n",
    "# crop the variables to the Southern Ocean latitude range: (40 ~ 85^o S)\n",
    "\n",
    "variable_nas = ['gmt', 'SST', 'p_e', 'LTS', 'SUB', 'LWP', 'LWP_statistic_error', 'rsdt', 'rsut', 'rsutcs', 'albedo', 'albedo_cs', 'alpha_cre', 'Maskarray_mac']\n",
    "\n",
    "dict1_SO, lat_merra2_so, lon_merra2_so = region_cropping(dict0_var, ['SST', 'p_e', 'LTS', 'SUB'], inputVar_obs['lat_merra2'], inputVar_obs['lon_merra2'], lat_range = [-85., -40.], lon_range = [-180., 180.])\n",
    "\n",
    "dict1_SO, lat_mac_so, lon_mac_so = region_cropping(dict1_SO, ['LWP', 'LWP_statistic_error', 'Maskarray_mac'], inputVar_obs['lat_mac'], inputVar_obs['lon_mac'], lat_range =[-85., -40.], lon_range = [-180., 180.])\n",
    "\n",
    "\n",
    "# Time-scale average\n",
    "# monthly mean (not changed)\n",
    "dict2_SO_mon = deepcopy(dict1_SO)\n",
    "\n",
    "# annually mean variable\n",
    "dict2_SO_yr = get_annually_dict(dict1_SO, ['gmt', 'SST', 'p_e', 'LTS', 'SUB', 'LWP', 'LWP_statistic_error', 'rsdt', 'rsut', 'rsutcs', 'albedo', 'albedo_cs', 'alpha_cre'], inputVar_obs['times_merra2'], label = 'mon')\n",
    "\n",
    "# binned (spatial) avergae.\n",
    "# Southern Ocean 5 * 5 degree bin box\n",
    "\n",
    "#..set are-mean range and define function\n",
    "s_range = arange(-90., 90., 5.) + 2.5  #..global-region latitude edge: (36)\n",
    "x_range = arange(-180., 180., 5.)  #..logitude sequences edge: number: 72\n",
    "y_range = arange(-85, -40., 5.) +2.5  #..southern-ocaen latitude edge: 9\n",
    "# binned Monthly variables:\n",
    "dict3_SO_mon_bin = {}\n",
    "\n",
    "for c in range(len(variable_nas)):\n",
    "\n",
    "    dict3_SO_mon_bin[variable_nas[c]] = binned_cySouthOcean5(dict2_SO_mon[variable_nas[c]], lat_merra2_so, lon_merra2_so)\n",
    "\n",
    "dict3_SO_mon_bin['gmt'] = binned_cyGlobal5(dict2_SO_mon['gmt'], inputVar_obs['lat_merra2'], lon_merra2_so)\n",
    "print(\"End monthly data binned.\")\n",
    "\n",
    "# binned Annually data:\n",
    "dict3_SO_yr_bin = get_annually_dict(dict3_SO_mon_bin, ['gmt', 'SST', 'p_e', 'LTS', 'SUB'], inputVar_obs['times_merra2'])\n",
    "print(\"End annually data binned.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL-3.7.9",
   "language": "python",
   "name": "npl-3.7.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
